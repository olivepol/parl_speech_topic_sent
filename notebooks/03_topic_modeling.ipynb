{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6035ab8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data loaded!\n",
      "Shape: (412, 23)\n",
      "\n",
      "Column names: ['id', 'session', 'electoralTerm', 'firstName', 'lastName', 'politicianId', 'speechContent', 'factionId', 'documentUrl', 'positionShort', 'positionLong', 'date', 'speech_length', 'paragraph_number', 'paragraph_length', 'tokens', 'token_count', 'tokens_no_stopwords', 'token_count_no_stopwords', 'tokens_clean', 'token_count_clean', 'tokens_lemma', 'token_count_lemma']\n",
      "\n",
      "Data types:\n",
      "Schema([('id', Int64), ('session', Int64), ('electoralTerm', Int64), ('firstName', String), ('lastName', String), ('politicianId', Int64), ('speechContent', String), ('factionId', Int64), ('documentUrl', String), ('positionShort', String), ('positionLong', String), ('date', String), ('speech_length', Int64), ('paragraph_number', Int64), ('paragraph_length', Int64), ('tokens', List(String)), ('token_count', UInt32), ('tokens_no_stopwords', List(String)), ('token_count_no_stopwords', UInt32), ('tokens_clean', List(String)), ('token_count_clean', UInt32), ('tokens_lemma', List(String)), ('token_count_lemma', UInt32)])\n",
      "\n",
      "First few rows:\n",
      "shape: (5, 23)\n",
      "┌────────┬─────────┬────────────┬───────────┬───┬────────────┬────────────┬────────────┬───────────┐\n",
      "│ id     ┆ session ┆ electoralT ┆ firstName ┆ … ┆ tokens_cle ┆ token_coun ┆ tokens_lem ┆ token_cou │\n",
      "│ ---    ┆ ---     ┆ erm        ┆ ---       ┆   ┆ an         ┆ t_clean    ┆ ma         ┆ nt_lemma  │\n",
      "│ i64    ┆ i64     ┆ ---        ┆ str       ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---       │\n",
      "│        ┆         ┆ i64        ┆           ┆   ┆ list[str]  ┆ u32        ┆ list[str]  ┆ u32       │\n",
      "╞════════╪═════════╪════════════╪═══════════╪═══╪════════════╪════════════╪════════════╪═══════════╡\n",
      "│ 738998 ┆ 43      ┆ 17         ┆ burkhard  ┆ … ┆ [\"Frau\",   ┆ 5          ┆ [\"Frau\",   ┆ 5         │\n",
      "│        ┆         ┆            ┆           ┆   ┆ \"Präsident ┆            ┆ \"Präsident ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆ in\", …     ┆            ┆ in\", …     ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆ \"Her…      ┆            ┆ \"Her…      ┆           │\n",
      "│ 738998 ┆ 43      ┆ 17         ┆ burkhard  ┆ … ┆ [\"Kollege\" ┆ 16         ┆ [\"Kollege\" ┆ 16        │\n",
      "│        ┆         ┆            ┆           ┆   ┆ , \"Hevelin ┆            ┆ , \"Hevelin ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆ g\", …      ┆            ┆ g\", …      ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆ \"bes…      ┆            ┆ \"bes…      ┆           │\n",
      "│ 738998 ┆ 43      ┆ 17         ┆ burkhard  ┆ … ┆ [\"Anlass\", ┆ 33         ┆ [\"Anlass\", ┆ 33        │\n",
      "│        ┆         ┆            ┆           ┆   ┆ \"heutige\", ┆            ┆ \"heutig\",  ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆ … \"stärk…  ┆            ┆ … \"stärke… ┆           │\n",
      "│ 738998 ┆ 43      ┆ 17         ┆ burkhard  ┆ … ┆ [\"Zahlen\", ┆ 17         ┆ [\"Zahl\",   ┆ 17        │\n",
      "│        ┆         ┆            ┆           ┆   ┆ \"wissen\",  ┆            ┆ \"wissen\",  ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆ … \"extrem… ┆            ┆ …          ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆            ┆ \"extrem\"]  ┆           │\n",
      "│ 738998 ┆ 43      ┆ 17         ┆ burkhard  ┆ … ┆ [\"Ermittlu ┆ 11         ┆ [\"Ermittlu ┆ 11        │\n",
      "│        ┆         ┆            ┆           ┆   ┆ ngsstelle\" ┆            ┆ ngsstelle\" ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆ ,          ┆            ┆ ,          ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆ \"Kinderp…  ┆            ┆ \"Kinderp…  ┆           │\n",
      "└────────┴─────────┴────────────┴───────────┴───┴────────────┴────────────┴────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "# Load preprocessed data\n",
    "data_file = Path('../data/processed/df_sample_split_preprocessed_topic.parquet')\n",
    "df = pl.read_parquet(data_file)\n",
    "\n",
    "print(f\"Preprocessed data loaded!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumn names: {df.columns}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.schema)\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f49fe910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens joined into full strings!\n",
      "\n",
      "Dataframe shape: (412, 26)\n",
      "New columns: ['id', 'session', 'electoralTerm', 'firstName', 'lastName', 'politicianId', 'speechContent', 'factionId', 'documentUrl', 'positionShort', 'positionLong', 'date', 'speech_length', 'paragraph_number', 'paragraph_length', 'tokens', 'token_count', 'tokens_no_stopwords', 'token_count_no_stopwords', 'tokens_clean', 'token_count_clean', 'tokens_lemma', 'token_count_lemma', 'text_lemmatized', 'text_clean', 'text_no_stopwords']\n",
      "\n",
      "Sample text from lemmatized tokens (first paragraph):\n",
      "  Frau Präsidentin Dame Herr Herr\n"
     ]
    }
   ],
   "source": [
    "# Join tokens back into full strings using native Polars (much faster than map_elements)\n",
    "import polars as pl\n",
    "\n",
    "# Use native Polars list.join() instead of slow map_elements\n",
    "df = df.with_columns(\n",
    "    pl.col('tokens_lemma').list.join(' ').alias('text_lemmatized')\n",
    ")\n",
    "\n",
    "# Also create versions from other token types for comparison\n",
    "df = df.with_columns(\n",
    "    pl.col('tokens_clean').list.join(' ').alias('text_clean')\n",
    ")\n",
    "\n",
    "df = df.with_columns(\n",
    "    pl.col('tokens_no_stopwords').list.join(' ').alias('text_no_stopwords')\n",
    ")\n",
    "\n",
    "print(\"Tokens joined into full strings!\")\n",
    "print(f\"\\nDataframe shape: {df.shape}\")\n",
    "print(f\"New columns: {df.columns}\")\n",
    "\n",
    "print(f\"\\nSample text from lemmatized tokens (first paragraph):\")\n",
    "print(f\"  {df['text_lemmatized'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3b8c4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorization\n",
      "Converting lemmatized texts to TF-IDF vectors...\n",
      "\n",
      "TF-IDF Vectorization complete!\n",
      "TF-IDF matrix shape: (412, 1000)\n",
      "  Samples (documents): 412\n",
      "  Features (vocabulary): 1000\n",
      "  Sparsity: 98.49%\n",
      "\n",
      "Vocabulary size: 1000\n",
      "Sample features: ['abgeordneter' 'abgeordneter dr' 'abgeordneter frau' 'abs' 'abschließen'\n",
      " 'abschließend' 'absehen' 'absolut' 'abstimmung' 'aktiv' 'aktuell' 'all'\n",
      " 'allgemein' 'alt' 'alternative' 'amerikanisch' 'amnesty'\n",
      " 'amnesty international' 'anbieten' 'anderer']\n",
      "\n",
      "Top 10 TF-IDF terms for first document:\n",
      "  präsidentin: 0.4746\n",
      "  frau präsidentin: 0.4746\n",
      "  herr: 0.4165\n",
      "  frau: 0.4011\n",
      "  dame herr: 0.3279\n",
      "  dame: 0.3279\n",
      "  überzeugung: 0.0000\n",
      "  überweisungsvorschlag: 0.0000\n",
      "  überprüfung: 0.0000\n",
      "  ansatz: 0.0000\n",
      "\n",
      "TF-IDF vectorizer and matrix ready for topic modeling!\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorization of lemmatized texts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "print(\"TF-IDF Vectorization\")\n",
    "print(\"Converting lemmatized texts to TF-IDF vectors...\")\n",
    "\n",
    "# Initialize TfidfVectorizer with German-specific parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,          # Limit vocabulary to top 1000 features\n",
    "    min_df=2,                   # Minimum document frequency\n",
    "    max_df=0.8,                 # Maximum document frequency (80% of docs)\n",
    "    ngram_range=(1, 2),         # Use unigrams and bigrams\n",
    "    sublinear_tf=True,          # Apply sublinear term frequency scaling\n",
    "    norm='l2'                   # L2 normalization\n",
    ")\n",
    "\n",
    "# Convert lemmatized texts to TF-IDF vectors\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text_lemmatized'])\n",
    "\n",
    "print(f\"\\nTF-IDF Vectorization complete!\")\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"  Samples (documents): {tfidf_matrix.shape[0]}\")\n",
    "print(f\"  Features (vocabulary): {tfidf_matrix.shape[1]}\")\n",
    "print(f\"  Sparsity: {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%\")\n",
    "\n",
    "# Get feature names (vocabulary)\n",
    "feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "print(f\"\\nVocabulary size: {len(feature_names)}\")\n",
    "print(f\"Sample features: {feature_names[:20]}\")\n",
    "\n",
    "# Convert sparse matrix to dense for inspection\n",
    "tfidf_dense = tfidf_matrix.toarray()\n",
    "\n",
    "# Show top TF-IDF terms for first document\n",
    "print(f\"\\nTop 10 TF-IDF terms for first document:\")\n",
    "top_indices = tfidf_dense[0].argsort()[-10:][::-1]\n",
    "for idx in top_indices:\n",
    "    print(f\"  {feature_names[idx]}: {tfidf_dense[0, idx]:.4f}\")\n",
    "\n",
    "# Store TF-IDF matrix and vectorizer for later use\n",
    "print(f\"\\nTF-IDF vectorizer and matrix ready for topic modeling!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0388207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topic Modeling\n",
      "Training LDA model with 20 topics...\n",
      "\n",
      "Document-Term Matrix created!\n",
      "Matrix shape: (412, 147)\n",
      "  Documents: 412\n",
      "  Terms (vocabulary): 147\n",
      "  Sparsity: 95.95%\n",
      "\n",
      "Training LDA model with 30 topics...\n",
      "iteration: 1 of max_iter: 30\n",
      "iteration: 2 of max_iter: 30\n",
      "iteration: 3 of max_iter: 30\n",
      "iteration: 4 of max_iter: 30\n",
      "iteration: 5 of max_iter: 30\n",
      "iteration: 6 of max_iter: 30\n",
      "iteration: 7 of max_iter: 30\n",
      "iteration: 8 of max_iter: 30\n",
      "iteration: 9 of max_iter: 30\n",
      "iteration: 10 of max_iter: 30\n",
      "iteration: 11 of max_iter: 30\n",
      "iteration: 12 of max_iter: 30\n",
      "iteration: 13 of max_iter: 30\n",
      "iteration: 14 of max_iter: 30\n",
      "iteration: 15 of max_iter: 30\n",
      "iteration: 16 of max_iter: 30\n",
      "iteration: 17 of max_iter: 30\n",
      "iteration: 18 of max_iter: 30\n",
      "iteration: 19 of max_iter: 30\n",
      "iteration: 20 of max_iter: 30\n",
      "iteration: 21 of max_iter: 30\n",
      "iteration: 22 of max_iter: 30\n",
      "iteration: 23 of max_iter: 30\n",
      "iteration: 24 of max_iter: 30\n",
      "iteration: 25 of max_iter: 30\n",
      "iteration: 26 of max_iter: 30\n",
      "iteration: 27 of max_iter: 30\n",
      "iteration: 28 of max_iter: 30\n",
      "iteration: 29 of max_iter: 30\n",
      "iteration: 30 of max_iter: 30\n",
      "\n",
      "LDA Model Training Complete!\n",
      "Model parameters:\n",
      "  Number of topics: 30\n",
      "  Number of iterations: 30\n",
      "  Perplexity: 195.2157\n",
      "  Score: -18628.1401\n",
      "\n",
      "================================================================================\n",
      "TOP 10 TERMS FOR EACH TOPIC\n",
      "================================================================================\n",
      "\n",
      "Topic  1: unser, haus, weg, nehmen, antwort, erwarten, kollegin, beratung, rede, hoch\n",
      "\n",
      "Topic  2: thema, letzter, unterschiedlich, ergebnis, punkt, abgeordneter, halten, herr, kollege, insbesondere\n",
      "\n",
      "Topic  3: zukunft, aufgabe, eigentlich, politisch, entsprechend, kollege, herr bundeskanzler, mensch, bundeskanzler, sehen\n",
      "\n",
      "Topic  4: deutsch, deutschland, geben, dm, haus, dr, stellen, klar, herr, arbeiten\n",
      "\n",
      "Topic  5: insbesondere, gebiet, richtig, lassen, erreichen, auffassung, interesse, bitte, bereich, hoch\n",
      "\n",
      "Topic  6: stehen, zeigen, verfügung, sinn, haus, ziel, rede, bund, richtig, stark\n",
      "\n",
      "Topic  7: glauben, tun, ding, million, fallen, punkt, anderer, sache, unseren, entscheiden\n",
      "\n",
      "Topic  8: mensch, können, notwendig, sinn, land, grund, handeln, art, frage, entscheidend\n",
      "\n",
      "Topic  9: deutsch, fall, einheit, kommen, nehmen, unseren, dm, mensch, verfügung, gut\n",
      "\n",
      "Topic 10: genau, antrag, frau, kollegin, kollege, spd, schaffen, glauben, einheit, geben\n",
      "\n",
      "Topic 11: herr, notwendig, bundeskanzler, art, herr bundeskanzler, glauben, handeln, sagen, satz, falsch\n",
      "\n",
      "Topic 12: frage, herr, staatssekretär, abgeordneter, kollege, dr, bundesregierung, herr kollege, antwort, übrig\n",
      "\n",
      "Topic 13: gesellschaft, sagen, kollege, gemeinsam, zukunft, wissen, land, herr, problem, herr kollege\n",
      "\n",
      "Topic 14: bund, brauchen, antrag, fraktion, letzter, satz, bundesrepublik, dm, spd, ausschuß\n",
      "\n",
      "Topic 15: weg, bringen, punkt, entscheidend, arbeiten, hoch, problem, deutschland, zeigen, frau\n",
      "\n",
      "Topic 16: deutsche, bundestag, deutsche bundestag, sprechen, regierung, entscheidend, ding, unser, müssen, rede\n",
      "\n",
      "Topic 17: mensch, europa, darstellen, führen, politisch, antwort, bundesregierung, leben, deutsche, punkt\n",
      "\n",
      "Topic 18: frage, deutschland, antwort, sagen, abgeordneter, führen, herr, kollege, hoch, tun\n",
      "\n",
      "Topic 19: arbeit, spd, genau, präsident, frau, bitte, unterschiedlich, sehen, deutschland, gut\n",
      "\n",
      "Topic 20: deutlich, liegen, kind, stelle, ziel, sagen, bundestag, notwendig, geben, feststellen\n",
      "\n",
      "Topic 21: frau, ausschuß, fraktion, beratung, abgeordneter, dr, antrag, gesetz, bericht, bundesregierung\n",
      "\n",
      "Topic 22: herr, liegen, minister, grund, präsident, erwarten, herr präsident, ergebnis, lage, unterschiedlich\n",
      "\n",
      "Topic 23: wort, kollege, fraktion, erreichen, unser, deutsche bundestag, zeigen, ding, leben, wichtig\n",
      "\n",
      "Topic 24: land, bundesregierung, regelung, gesetz, unser, regierung, bedeuten, unser land, unseren, ziel\n",
      "\n",
      "Topic 25: frau, darstellen, dr, ausschuß, fraktion, abgeordneter, beratung, herr, mensch, arbeit\n",
      "\n",
      "Topic 26: million, kommen, brauchen, entscheiden, deutsche, dm, stellen, verfügung, deutsche bundestag, wort\n",
      "\n",
      "Topic 27: kind, brauchen, frage, antwort, partei, aufgabe, situation, unseren, gehören, stellen\n",
      "\n",
      "Topic 28: deutsche bundestag, situation, spd, verfügung, bedeuten, seite, erkennen, nehmen, übrig, unterschiedlich\n",
      "\n",
      "Topic 29: beratung, verfügung, sagen, liegen, präsident, bundesrepublik, bereich, falsch, unser, erreichen\n",
      "\n",
      "Topic 30: herr, dame herr, dame, politisch, sagen, sehen, kollege, bundesregierung, präsident, frage\n",
      "\n",
      "================================================================================\n",
      "LDA Model ready for analysis!\n"
     ]
    }
   ],
   "source": [
    "# LDA Topic Modeling with optimized parameters\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "print(\"LDA Topic Modeling\")\n",
    "print(\"Training LDA model with 20 topics...\")\n",
    "\n",
    "# LDA requires CountVectorizer, not TF-IDF\n",
    "# Create CountVectorizer with the specified parameters\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=1000,\n",
    "    min_df=10,               # Minimum document frequency (seltene Wörter raus)\n",
    "    max_df=0.90,            # Maximum document frequency (sehr häufige Wörter raus)\n",
    "    ngram_range=(1, 2),     # Unigrams and bigrams\n",
    "    stop_words='english'    # Basic English stopwords (additional filter)\n",
    ")\n",
    "\n",
    "# Fit and transform the texts\n",
    "doc_term_matrix = count_vectorizer.fit_transform(df['text_lemmatized'])\n",
    "\n",
    "print(f\"\\nDocument-Term Matrix created!\")\n",
    "print(f\"Matrix shape: {doc_term_matrix.shape}\")\n",
    "print(f\"  Documents: {doc_term_matrix.shape[0]}\")\n",
    "print(f\"  Terms (vocabulary): {doc_term_matrix.shape[1]}\")\n",
    "print(f\"  Sparsity: {(1 - doc_term_matrix.nnz / (doc_term_matrix.shape[0] * doc_term_matrix.shape[1])) * 100:.2f}%\")\n",
    "\n",
    "# Initialize and train LDA model\n",
    "n_topics = 30\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    random_state=42,\n",
    "    max_iter=30,\n",
    "    learning_method='online',\n",
    "    n_jobs=-1,              # Use all available cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining LDA model with {n_topics} topics...\")\n",
    "lda_model.fit(doc_term_matrix)\n",
    "\n",
    "print(f\"\\nLDA Model Training Complete!\")\n",
    "print(f\"Model parameters:\")\n",
    "print(f\"  Number of topics: {n_topics}\")\n",
    "print(f\"  Number of iterations: {lda_model.max_iter}\")\n",
    "print(f\"  Perplexity: {lda_model.perplexity(doc_term_matrix):.4f}\")\n",
    "print(f\"  Score: {lda_model.score(doc_term_matrix):.4f}\")\n",
    "\n",
    "# Get feature names\n",
    "feature_names = np.array(count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display top terms for each topic\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 10 TERMS FOR EACH TOPIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "n_top_words = 10\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_words_idx = topic.argsort()[-n_top_words:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"\\nTopic {topic_idx + 1:2d}: {', '.join(top_words)}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"LDA Model ready for analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9db7887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from gensim) (2.3.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from gensim) (1.16.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "967be269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score Berechnung\n",
      "Calculating coherence metrics for the LDA model...\n",
      "\n",
      "Perplexity Score: 195.2157\n",
      "  (Interpretation: niedriger ist besser)\n",
      "\n",
      "================================================================================\n",
      "COHERENCE SCORES\n",
      "================================================================================\n",
      "\n",
      "Average Topic Coherence: 0.2386\n",
      "  (Range: 0-1, höher ist besser. >0.5 ist akzeptabel)\n",
      "\n",
      "Coherence scores by topic:\n",
      "  Topic  1: 0.1908\n",
      "  Topic  2: 0.2021\n",
      "  Topic  3: 0.1798\n",
      "  Topic  4: 0.2491\n",
      "  Topic  5: 0.3895\n",
      "  Topic  6: 0.1731\n",
      "  Topic  7: 0.2179\n",
      "  Topic  8: 0.2304\n",
      "  Topic  9: 0.1315\n",
      "  Topic 10: 0.2246\n",
      "  Topic 11: 0.2804\n",
      "  Topic 12: 0.2359\n",
      "  Topic 13: 0.2697\n",
      "  Topic 14: 0.1731\n",
      "  Topic 15: 0.1509\n",
      "  Topic 16: 0.3098\n",
      "  Topic 17: 0.2410\n",
      "  Topic 18: 0.2432\n",
      "  Topic 19: 0.1295\n",
      "  Topic 20: 0.2375\n",
      "  Topic 21: 0.3918\n",
      "  Topic 22: 0.2812\n",
      "  Topic 23: 0.2027\n",
      "  Topic 24: 0.3143\n",
      "  Topic 25: 0.3473\n",
      "  Topic 26: 0.1716\n",
      "  Topic 27: 0.1568\n",
      "  Topic 28: 0.1710\n",
      "  Topic 29: 0.2150\n",
      "  Topic 30: 0.4466\n",
      "\n",
      "Log-Likelihood per document: -45.2139\n",
      "\n",
      "================================================================================\n",
      "MODEL EVALUATION SUMMARY\n",
      "================================================================================\n",
      "Number of Topics: 20\n",
      "Number of Documents: 412\n",
      "Vocabulary Size: 147\n",
      "\n",
      "Perplexity: 195.2157\n",
      "Average Topic Coherence: 0.2386\n",
      "Log-Likelihood: -18628.1401\n",
      "\n",
      "Interpretation: ✗ Modell könnte verbessert werden\n"
     ]
    }
   ],
   "source": [
    "# Coherence Score Berechnung\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "print(\"Coherence Score Berechnung\")\n",
    "print(\"Calculating coherence metrics for the LDA model...\")\n",
    "\n",
    "# Calculate Perplexity\n",
    "perplexity = lda_model.perplexity(doc_term_matrix)\n",
    "print(f\"\\nPerplexity Score: {perplexity:.4f}\")\n",
    "print(f\"  (Interpretation: niedriger ist besser)\")\n",
    "\n",
    "# Calculate topic coherence manually\n",
    "# Method: Measure similarity between top words in each topic\n",
    "def calculate_topic_coherence(lda_model, doc_term_matrix, feature_names, top_n=10):\n",
    "    \"\"\"\n",
    "    Calculate coherence of topics based on co-occurrence of top words\n",
    "    \"\"\"\n",
    "    # Convert to dense for easier computation\n",
    "    dtm_dense = doc_term_matrix.toarray()\n",
    "    \n",
    "    coherence_scores = []\n",
    "    \n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        # Get top words for this topic\n",
    "        top_word_indices = topic.argsort()[-top_n:][::-1]\n",
    "        \n",
    "        # Calculate pairwise similarity between top words based on document co-occurrence\n",
    "        word_vectors = dtm_dense[:, top_word_indices]\n",
    "        \n",
    "        # Calculate cosine similarity between word vectors\n",
    "        if word_vectors.shape[1] > 1:\n",
    "            similarity_matrix = cosine_similarity(word_vectors.T)\n",
    "            # Get average similarity (excluding diagonal)\n",
    "            np.fill_diagonal(similarity_matrix, 0)\n",
    "            avg_similarity = similarity_matrix.sum() / (word_vectors.shape[1] * (word_vectors.shape[1] - 1))\n",
    "            coherence_scores.append(avg_similarity)\n",
    "    \n",
    "    return np.mean(coherence_scores), coherence_scores\n",
    "\n",
    "# Get feature names from count vectorizer\n",
    "feature_names = np.array(count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Calculate coherence\n",
    "avg_coherence, topic_coherences = calculate_topic_coherence(lda_model, doc_term_matrix, feature_names)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"COHERENCE SCORES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAverage Topic Coherence: {avg_coherence:.4f}\")\n",
    "print(f\"  (Range: 0-1, höher ist besser. >0.5 ist akzeptabel)\")\n",
    "\n",
    "# Show individual topic coherence scores\n",
    "print(f\"\\nCoherence scores by topic:\")\n",
    "for topic_idx, coherence in enumerate(topic_coherences):\n",
    "    print(f\"  Topic {topic_idx + 1:2d}: {coherence:.4f}\")\n",
    "\n",
    "# Calculate log-likelihood per document\n",
    "log_likelihood = lda_model.score(doc_term_matrix)\n",
    "print(f\"\\nLog-Likelihood per document: {log_likelihood / doc_term_matrix.shape[0]:.4f}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Number of Topics: 20\")\n",
    "print(f\"Number of Documents: {doc_term_matrix.shape[0]}\")\n",
    "print(f\"Vocabulary Size: {doc_term_matrix.shape[1]}\")\n",
    "print(f\"\\nPerplexity: {perplexity:.4f}\")\n",
    "print(f\"Average Topic Coherence: {avg_coherence:.4f}\")\n",
    "print(f\"Log-Likelihood: {log_likelihood:.4f}\")\n",
    "print(f\"\\nInterpretation: {'✓ Gutes Modell' if avg_coherence > 0.5 else '✗ Modell könnte verbessert werden'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c43edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAVING LDA TOPIC MODEL RESULTS\n",
      "================================================================================\n",
      "\n",
      "1. Extracting topic assignments for each document...\n",
      "Document-topic assignments shape: (412, 17)\n",
      "Columns: ['id', 'session', 'electoralTerm', 'firstName', 'lastName', 'politicianId', 'factionId', 'documentUrl', 'positionShort', 'positionLong', 'date', 'speech_length', 'paragraph_number', 'paragraph_length', 'speechContent', 'dominant_topic', 'dominant_topic_prob']\n",
      "\n",
      "First 10 rows:\n",
      "shape: (10, 17)\n",
      "┌────────┬─────────┬────────────┬───────────┬───┬────────────┬────────────┬────────────┬───────────┐\n",
      "│ id     ┆ session ┆ electoralT ┆ firstName ┆ … ┆ paragraph_ ┆ speechCont ┆ dominant_t ┆ dominant_ │\n",
      "│ ---    ┆ ---     ┆ erm        ┆ ---       ┆   ┆ length     ┆ ent        ┆ opic       ┆ topic_pro │\n",
      "│ i64    ┆ i64     ┆ ---        ┆ str       ┆   ┆ ---        ┆ ---        ┆ ---        ┆ b         │\n",
      "│        ┆         ┆ i64        ┆           ┆   ┆ i64        ┆ str        ┆ i64        ┆ ---       │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆            ┆            ┆ f64       │\n",
      "╞════════╪═════════╪════════════╪═══════════╪═══╪════════════╪════════════╪════════════╪═══════════╡\n",
      "│ 738998 ┆ 43      ┆ 17         ┆ burkhard  ┆ … ┆ 46         ┆ Frau Präsi ┆ 29         ┆ 0.672193  │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ dentin!    ┆            ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ Meine      ┆            ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ Damen …    ┆            ┆           │\n",
      "│ 738998 ┆ 43      ┆ 17         ┆ burkhard  ┆ … ┆ 317        ┆ Kollege    ┆ 12         ┆ 0.724805  │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ Heveling,  ┆            ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ ich bin    ┆            ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ auf …      ┆            ┆           │\n",
      "│ 738998 ┆ 43      ┆ 17         ┆ burkhard  ┆ … ┆ 588        ┆ Der Anlass ┆ 26         ┆ 0.892593  │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ für unsere ┆            ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ heutige …  ┆            ┆           │\n",
      "│ 738998 ┆ 43      ┆ 17         ┆ burkhard  ┆ … ┆ 264        ┆ Zu den     ┆ 12         ┆ 0.505556  │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ Zahlen.    ┆            ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ Sie        ┆            ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ wissen, 12 ┆            ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ …          ┆            ┆           │\n",
      "│ 738998 ┆ 43      ┆ 17         ┆ burkhard  ┆ … ┆ 163        ┆ Allein die ┆ 25         ┆ 0.677778  │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ Ermittlung ┆            ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ sstelle K… ┆            ┆           │\n",
      "│ 738998 ┆ 43      ┆ 17         ┆ burkhard  ┆ … ┆ 414        ┆ Jetzt      ┆ 19         ┆ 0.730793  │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ liegt ein  ┆            ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ Richtlinie ┆            ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ nvor…      ┆            ┆           │\n",
      "│ 738998 ┆ 43      ┆ 17         ┆ burkhard  ┆ … ┆ 120        ┆ Das erste  ┆ 1          ┆ 0.516667  │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ Thema ist  ┆            ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ schon      ┆            ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ ange…      ┆            ┆           │\n",
      "│ 738998 ┆ 43      ┆ 17         ┆ burkhard  ┆ … ┆ 5          ┆ ({0})      ┆ 0          ┆ 0.033333  │\n",
      "│ 738998 ┆ 43      ┆ 17         ┆ burkhard  ┆ … ┆ 493        ┆ Wir wollen ┆ 29         ┆ 0.475845  │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ Kinderporn ┆            ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆            ┆ ografie i… ┆            ┆           │\n",
      "│ 738998 ┆ 43      ┆ 17         ┆ burkhard  ┆ … ┆ 5          ┆ ({1})      ┆ 0          ┆ 0.033333  │\n",
      "└────────┴─────────┴────────────┴───────────┴───┴────────────┴────────────┴────────────┴───────────┘\n",
      "\n",
      "2. Extracting top terms for each topic...\n",
      "Topics-terms dataframe shape: (30, 4)\n",
      "\n",
      "First 5 topics:\n",
      "shape: (5, 2)\n",
      "┌───────┬─────────────────────────────────┐\n",
      "│ topic ┆ top_terms                       │\n",
      "│ ---   ┆ ---                             │\n",
      "│ i64   ┆ str                             │\n",
      "╞═══════╪═════════════════════════════════╡\n",
      "│ 0     ┆ unser, haus, weg, nehmen, antw… │\n",
      "│ 1     ┆ thema, letzter, unterschiedlic… │\n",
      "│ 2     ┆ zukunft, aufgabe, eigentlich, … │\n",
      "│ 3     ┆ deutsch, deutschland, geben, d… │\n",
      "│ 4     ┆ insbesondere, gebiet, richtig,… │\n",
      "└───────┴─────────────────────────────────┘\n",
      "\n",
      "3. Creating topic quality metrics...\n",
      "Topic metrics shape: (30, 2)\n",
      "\n",
      "Topic metrics:\n",
      "shape: (5, 2)\n",
      "┌───────┬─────────────────┐\n",
      "│ topic ┆ coherence_score │\n",
      "│ ---   ┆ ---             │\n",
      "│ i64   ┆ f64             │\n",
      "╞═══════╪═════════════════╡\n",
      "│ 0     ┆ 0.190793        │\n",
      "│ 1     ┆ 0.202097        │\n",
      "│ 2     ┆ 0.179761        │\n",
      "│ 3     ┆ 0.249089        │\n",
      "│ 4     ┆ 0.389518        │\n",
      "└───────┴─────────────────┘\n",
      "\n",
      "✓ LDA Document-topic assignments saved (with all metadata):\n",
      "  Parquet: ..\\data\\processed\\topic_document_assignments_lda.parquet\n",
      "  CSV: ..\\data\\processed\\topic_document_assignments_lda.csv\n",
      "  Total columns: 17\n",
      "\n",
      "✓ LDA Topic terms saved:\n",
      "  Parquet: ..\\data\\processed\\topic_terms_lda.parquet\n",
      "  CSV: ..\\data\\processed\\topic_terms_lda.csv\n",
      "\n",
      "✓ LDA Topic metrics saved:\n",
      "  Parquet: ..\\data\\processed\\topic_metrics_lda.parquet\n",
      "  CSV: ..\\data\\processed\\topic_metrics_lda.csv\n",
      "\n",
      "================================================================================\n",
      "LDA TOPIC MODEL RESULTS SAVED SUCCESSFULLY!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save LDA Topic Model Results to DataFrames using Polars (with all metadata)\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING LDA TOPIC MODEL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Get topic assignments for each document\n",
    "print(\"\\n1. Extracting topic assignments for each document...\")\n",
    "doc_topic_dist = lda_model.transform(doc_term_matrix)  # Shape: (n_docs, n_topics)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topics = doc_topic_dist.argmax(axis=1)\n",
    "dominant_topic_prob = doc_topic_dist.max(axis=1)\n",
    "\n",
    "# Get all metadata columns from original dataframe\n",
    "metadata_cols = ['id', 'session', 'electoralTerm', 'firstName', 'lastName', 'politicianId', \n",
    "                 'factionId', 'documentUrl', 'positionShort', 'positionLong', 'date', \n",
    "                 'speech_length', 'paragraph_number', 'paragraph_length', 'speechContent']\n",
    "df_metadata = df.select(metadata_cols)\n",
    "\n",
    "# Create Polars dataframe with document-level topic assignments and all metadata\n",
    "df_lda_topics_docs = df_metadata.with_columns(\n",
    "    pl.Series('dominant_topic', dominant_topics.tolist()),\n",
    "    pl.Series('dominant_topic_prob', dominant_topic_prob.tolist())\n",
    ")\n",
    "\n",
    "print(f\"Document-topic assignments shape: {df_lda_topics_docs.shape}\")\n",
    "print(f\"Columns: {df_lda_topics_docs.columns}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "print(df_lda_topics_docs.head(10))\n",
    "\n",
    "# 2. Get top terms for each topic\n",
    "print(f\"\\n2. Extracting top terms for each topic...\")\n",
    "n_top_words = 15\n",
    "topics_terms_list = []\n",
    "\n",
    "for topic_idx, topic_components in enumerate(lda_model.components_):\n",
    "    # Get indices of top words\n",
    "    top_words_idx = topic_components.argsort()[-n_top_words:][::-1]\n",
    "    \n",
    "    # Get words and their weights\n",
    "    top_words = feature_names[top_words_idx]\n",
    "    top_weights = topic_components[top_words_idx]\n",
    "    \n",
    "    topics_terms_list.append({\n",
    "        'topic': topic_idx,\n",
    "        'top_terms': ', '.join(top_words),\n",
    "        'top_terms_list': json.dumps(top_words.tolist()),\n",
    "        'weights': json.dumps(top_weights.tolist())\n",
    "    })\n",
    "\n",
    "df_lda_topics_terms = pl.DataFrame(topics_terms_list)\n",
    "\n",
    "print(f\"Topics-terms dataframe shape: {df_lda_topics_terms.shape}\")\n",
    "print(f\"\\nFirst 5 topics:\")\n",
    "print(df_lda_topics_terms.select(['topic', 'top_terms']).head())\n",
    "\n",
    "# 3. Create topic quality metrics dataframe\n",
    "print(f\"\\n3. Creating topic quality metrics...\")\n",
    "df_lda_topic_metrics = pl.DataFrame({\n",
    "    'topic': list(range(n_topics)),\n",
    "    'coherence_score': topic_coherences\n",
    "})\n",
    "\n",
    "print(f\"Topic metrics shape: {df_lda_topic_metrics.shape}\")\n",
    "print(f\"\\nTopic metrics:\")\n",
    "print(df_lda_topic_metrics.head())\n",
    "\n",
    "# 4. Save all LDA results to parquet and csv\n",
    "output_dir = Path('../data/processed')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save document-topic assignments with all metadata\n",
    "parquet_path_docs = output_dir / 'topic_document_assignments_lda.parquet'\n",
    "csv_path_docs = output_dir / 'topic_document_assignments_lda.csv'\n",
    "\n",
    "df_lda_topics_docs.write_parquet(parquet_path_docs)\n",
    "df_lda_topics_docs.write_csv(csv_path_docs)\n",
    "\n",
    "print(f\"\\n✓ LDA Document-topic assignments saved (with all metadata):\")\n",
    "print(f\"  Parquet: {parquet_path_docs}\")\n",
    "print(f\"  CSV: {csv_path_docs}\")\n",
    "print(f\"  Total columns: {df_lda_topics_docs.width}\")\n",
    "\n",
    "# Save topic terms\n",
    "parquet_path_terms = output_dir / 'topic_terms_lda.parquet'\n",
    "csv_path_terms = output_dir / 'topic_terms_lda.csv'\n",
    "\n",
    "df_lda_topics_terms.write_parquet(parquet_path_terms)\n",
    "df_lda_topics_terms.select(['topic', 'top_terms']).write_csv(csv_path_terms)\n",
    "\n",
    "print(f\"\\n✓ LDA Topic terms saved:\")\n",
    "print(f\"  Parquet: {parquet_path_terms}\")\n",
    "print(f\"  CSV: {csv_path_terms}\")\n",
    "\n",
    "# Save topic metrics\n",
    "parquet_path_metrics = output_dir / 'topic_metrics_lda.parquet'\n",
    "csv_path_metrics = output_dir / 'topic_metrics_lda.csv'\n",
    "\n",
    "df_lda_topic_metrics.write_parquet(parquet_path_metrics)\n",
    "df_lda_topic_metrics.write_csv(csv_path_metrics)\n",
    "\n",
    "print(f\"\\n✓ LDA Topic metrics saved:\")\n",
    "print(f\"  Parquet: {parquet_path_metrics}\")\n",
    "print(f\"  CSV: {csv_path_metrics}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"LDA TOPIC MODEL RESULTS SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465c40d9",
   "metadata": {},
   "source": [
    "# Alternative Topic Classification: ParlBERT-Topic-German\n",
    "\n",
    "The LDA topic modeling results above showed limited coherence and interpretability. The topics were not well-defined and difficult to assign meaningful labels.\n",
    "\n",
    "As an alternative, we use the **ParlBERT-Topic-German** model from HuggingFace ([chkla/parlbert-topic-german](https://huggingface.co/chkla/parlbert-topic-german)), which is a fine-tuned BERT model specifically trained for German parliamentary speech topic classification. This model classifies text into predefined political topic categories, providing more interpretable and consistent results for parliamentary speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "106caac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: torch in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\olive\\miniconda3\\envs\\parl_speech\\lib\\site-packages (from requests->transformers) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install transformers if not already installed\n",
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46a4be84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\olive\\miniconda3\\envs\\parl_speech\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PARLBERT-TOPIC-GERMAN CLASSIFICATION\n",
      "================================================================================\n",
      "\n",
      "Loading ParlBERT-Topic-German model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "c:\\Users\\olive\\miniconda3\\envs\\parl_speech\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully!\n",
      "\n",
      "Test classification:\n",
      "  Text: Das Sachgebiet Investive Ausgaben des Bundes Bundesfinanzminister Apel hat gemäß...\n",
      "  Predicted topic: Macroeconomics\n",
      "  Confidence: 0.9967\n"
     ]
    }
   ],
   "source": [
    "# ParlBERT-Topic-German Classification\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PARLBERT-TOPIC-GERMAN CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize the classification pipeline\n",
    "print(\"\\nLoading ParlBERT-Topic-German model...\")\n",
    "pipeline_classification_topics = pipeline(\n",
    "    \"text-classification\", \n",
    "    model=\"chkla/parlbert-topic-german\", \n",
    "    return_all_scores=False\n",
    ")\n",
    "print(\"✓ Model loaded successfully!\")\n",
    "\n",
    "# Test the model with an example\n",
    "test_text = \"Das Sachgebiet Investive Ausgaben des Bundes Bundesfinanzminister Apel hat gemäß BMF Finanznachrichten vom 1. Januar erklärt, die Investitionsquote des Bundes sei in den letzten zehn Jahren nahezu konstant geblieben.\"\n",
    "test_result = pipeline_classification_topics(test_text)\n",
    "print(f\"\\nTest classification:\")\n",
    "print(f\"  Text: {test_text[:80]}...\")\n",
    "print(f\"  Predicted topic: {test_result[0]['label']}\")\n",
    "print(f\"  Confidence: {test_result[0]['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ffabdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifying all documents...\n",
      "Total documents to classify: 412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying: 100%|██████████| 26/26 [01:31<00:00,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Classification complete!\n",
      "  Documents classified: 412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Classify all documents using ParlBERT\n",
    "print(\"\\nClassifying all documents...\")\n",
    "print(f\"Total documents to classify: {df.shape[0]}\")\n",
    "\n",
    "# Use the original speechContent for classification (not lemmatized)\n",
    "# The model expects natural German text\n",
    "texts = df['speechContent'].to_list()\n",
    "\n",
    "# Classify in batches for efficiency\n",
    "# Use truncation=True to let the tokenizer handle the 512 token limit properly\n",
    "results = []\n",
    "batch_size = 16  # Reduced batch size for stability\n",
    "\n",
    "for i in tqdm(range(0, len(texts), batch_size), desc=\"Classifying\"):\n",
    "    batch_texts = texts[i:i+batch_size]\n",
    "    # Handle None/empty texts\n",
    "    batch_texts_clean = [text if text else \"\" for text in batch_texts]\n",
    "    # Use truncation=True to properly handle the 512 token limit\n",
    "    batch_results = pipeline_classification_topics(batch_texts_clean, truncation=True, max_length=512)\n",
    "    results.extend(batch_results)\n",
    "\n",
    "# Extract labels and scores\n",
    "topic_labels = [r['label'] for r in results]\n",
    "topic_scores = [r['score'] for r in results]\n",
    "\n",
    "print(f\"\\n✓ Classification complete!\")\n",
    "print(f\"  Documents classified: {len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4aff8bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOPIC DISTRIBUTION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Topic distribution:\n",
      "  Government                    :   180 documents (43.69%)\n",
      "  Civil                         :    86 documents (20.87%)\n",
      "  International                 :    22 documents ( 5.34%)\n",
      "  Law                           :    19 documents ( 4.61%)\n",
      "  Macroeconomics                :    19 documents ( 4.61%)\n",
      "  Labor                         :    16 documents ( 3.88%)\n",
      "  Social                        :    15 documents ( 3.64%)\n",
      "  Defense                       :    11 documents ( 2.67%)\n",
      "  Agriculture                   :    10 documents ( 2.43%)\n",
      "  Domestic                      :     8 documents ( 1.94%)\n",
      "  Environment                   :     7 documents ( 1.70%)\n",
      "  Technology                    :     5 documents ( 1.21%)\n",
      "  Housing                       :     4 documents ( 0.97%)\n",
      "  Health                        :     4 documents ( 0.97%)\n",
      "  Education                     :     2 documents ( 0.49%)\n",
      "  Energy                        :     2 documents ( 0.49%)\n",
      "  Foreign                       :     1 documents ( 0.24%)\n",
      "  Transportation                :     1 documents ( 0.24%)\n",
      "\n",
      "Total unique topics: 18\n"
     ]
    }
   ],
   "source": [
    "# Analyze topic distribution\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOPIC DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get unique topics and their counts\n",
    "topic_counts = pl.DataFrame({'topic_label': topic_labels}).group_by('topic_label').len().sort('len', descending=True)\n",
    "\n",
    "print(\"\\nTopic distribution:\")\n",
    "for row in topic_counts.iter_rows(named=True):\n",
    "    percentage = (row['len'] / len(topic_labels)) * 100\n",
    "    print(f\"  {row['topic_label']:30s}: {row['len']:5d} documents ({percentage:5.2f}%)\")\n",
    "\n",
    "print(f\"\\nTotal unique topics: {topic_counts.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ec9e69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING PARLBERT TOPIC CLASSIFICATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "Document-topic assignments shape: (412, 18)\n",
      "Columns: ['id', 'session', 'electoralTerm', 'firstName', 'lastName', 'politicianId', 'factionId', 'documentUrl', 'positionShort', 'positionLong', 'date', 'speech_length', 'paragraph_number', 'paragraph_length', 'speechContent', 'dominant_topic', 'dominant_topic_prob', 'topic_label']\n",
      "\n",
      "First 10 rows (selected columns):\n",
      "shape: (10, 6)\n",
      "┌────────┬───────────┬──────────┬────────────────┬─────────────┬─────────────────────┐\n",
      "│ id     ┆ firstName ┆ lastName ┆ dominant_topic ┆ topic_label ┆ dominant_topic_prob │\n",
      "│ ---    ┆ ---       ┆ ---      ┆ ---            ┆ ---         ┆ ---                 │\n",
      "│ i64    ┆ str       ┆ str      ┆ i64            ┆ str         ┆ f64                 │\n",
      "╞════════╪═══════════╪══════════╪════════════════╪═════════════╪═════════════════════╡\n",
      "│ 738998 ┆ burkhard  ┆ lischka  ┆ 8              ┆ Government  ┆ 0.942619            │\n",
      "│ 738998 ┆ burkhard  ┆ lischka  ┆ 8              ┆ Government  ┆ 0.990203            │\n",
      "│ 738998 ┆ burkhard  ┆ lischka  ┆ 13             ┆ Law         ┆ 0.996563            │\n",
      "│ 738998 ┆ burkhard  ┆ lischka  ┆ 13             ┆ Law         ┆ 0.998165            │\n",
      "│ 738998 ┆ burkhard  ┆ lischka  ┆ 13             ┆ Law         ┆ 0.970507            │\n",
      "│ 738998 ┆ burkhard  ┆ lischka  ┆ 16             ┆ Technology  ┆ 0.606678            │\n",
      "│ 738998 ┆ burkhard  ┆ lischka  ┆ 16             ┆ Technology  ┆ 0.622835            │\n",
      "│ 738998 ┆ burkhard  ┆ lischka  ┆ 8              ┆ Government  ┆ 0.469457            │\n",
      "│ 738998 ┆ burkhard  ┆ lischka  ┆ 16             ┆ Technology  ┆ 0.668161            │\n",
      "│ 738998 ┆ burkhard  ┆ lischka  ┆ 1              ┆ Civil       ┆ 0.496788            │\n",
      "└────────┴───────────┴──────────┴────────────────┴─────────────┴─────────────────────┘\n",
      "\n",
      "Topics terms/labels dataframe shape: (18, 3)\n",
      "\n",
      "Topic mapping:\n",
      "shape: (18, 3)\n",
      "┌───────┬────────────────┬────────────────┐\n",
      "│ topic ┆ top_terms      ┆ topic_label    │\n",
      "│ ---   ┆ ---            ┆ ---            │\n",
      "│ i64   ┆ str            ┆ str            │\n",
      "╞═══════╪════════════════╪════════════════╡\n",
      "│ 0     ┆ Agriculture    ┆ Agriculture    │\n",
      "│ 1     ┆ Civil          ┆ Civil          │\n",
      "│ 2     ┆ Defense        ┆ Defense        │\n",
      "│ 3     ┆ Domestic       ┆ Domestic       │\n",
      "│ 4     ┆ Education      ┆ Education      │\n",
      "│ …     ┆ …              ┆ …              │\n",
      "│ 13    ┆ Law            ┆ Law            │\n",
      "│ 14    ┆ Macroeconomics ┆ Macroeconomics │\n",
      "│ 15    ┆ Social         ┆ Social         │\n",
      "│ 16    ┆ Technology     ┆ Technology     │\n",
      "│ 17    ┆ Transportation ┆ Transportation │\n",
      "└───────┴────────────────┴────────────────┘\n",
      "\n",
      "Topic metrics shape: (18, 5)\n",
      "\n",
      "Topic metrics:\n",
      "shape: (18, 5)\n",
      "┌───────┬────────────────┬─────────────────┬────────────────┬────────────────┐\n",
      "│ topic ┆ topic_label    ┆ mean_confidence ┆ std_confidence ┆ document_count │\n",
      "│ ---   ┆ ---            ┆ ---             ┆ ---            ┆ ---            │\n",
      "│ i64   ┆ str            ┆ f64             ┆ f64            ┆ u32            │\n",
      "╞═══════╪════════════════╪═════════════════╪════════════════╪════════════════╡\n",
      "│ 0     ┆ Agriculture    ┆ 0.934629        ┆ 0.131943       ┆ 10             │\n",
      "│ 1     ┆ Civil          ┆ 0.606794        ┆ 0.19318        ┆ 86             │\n",
      "│ 2     ┆ Defense        ┆ 0.956891        ┆ 0.115209       ┆ 11             │\n",
      "│ 3     ┆ Domestic       ┆ 0.944106        ┆ 0.077707       ┆ 8              │\n",
      "│ 4     ┆ Education      ┆ 0.997873        ┆ 0.000664       ┆ 2              │\n",
      "│ …     ┆ …              ┆ …               ┆ …              ┆ …              │\n",
      "│ 13    ┆ Law            ┆ 0.962178        ┆ 0.062031       ┆ 19             │\n",
      "│ 14    ┆ Macroeconomics ┆ 0.835497        ┆ 0.168765       ┆ 19             │\n",
      "│ 15    ┆ Social         ┆ 0.809505        ┆ 0.220771       ┆ 15             │\n",
      "│ 16    ┆ Technology     ┆ 0.694446        ┆ 0.172531       ┆ 5              │\n",
      "│ 17    ┆ Transportation ┆ 0.978443        ┆ null           ┆ 1              │\n",
      "└───────┴────────────────┴─────────────────┴────────────────┴────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Save ParlBERT Classification Results to DataFrames\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING PARLBERT TOPIC CLASSIFICATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get all metadata columns from original dataframe\n",
    "metadata_cols = ['id', 'session', 'electoralTerm', 'firstName', 'lastName', 'politicianId', \n",
    "                 'factionId', 'documentUrl', 'positionShort', 'positionLong', 'date', \n",
    "                 'speech_length', 'paragraph_number', 'paragraph_length', 'speechContent']\n",
    "df_metadata = df.select(metadata_cols)\n",
    "\n",
    "# Create numeric topic IDs for compatibility with existing analysis\n",
    "unique_topics = sorted(list(set(topic_labels)))\n",
    "topic_to_id = {topic: idx for idx, topic in enumerate(unique_topics)}\n",
    "topic_ids = [topic_to_id[label] for label in topic_labels]\n",
    "\n",
    "# Create Polars dataframe with document-level topic assignments and all metadata\n",
    "df_topics_docs = df_metadata.with_columns(\n",
    "    pl.Series('dominant_topic', topic_ids),\n",
    "    pl.Series('dominant_topic_prob', topic_scores),\n",
    "    pl.Series('topic_label', topic_labels)\n",
    ")\n",
    "\n",
    "print(f\"\\nDocument-topic assignments shape: {df_topics_docs.shape}\")\n",
    "print(f\"Columns: {df_topics_docs.columns}\")\n",
    "print(f\"\\nFirst 10 rows (selected columns):\")\n",
    "print(df_topics_docs.select(['id', 'firstName', 'lastName', 'dominant_topic', 'topic_label', 'dominant_topic_prob']).head(10))\n",
    "\n",
    "# Create topic terms/labels dataframe (mapping topic IDs to their labels)\n",
    "df_topics_terms = pl.DataFrame({\n",
    "    'topic': list(range(len(unique_topics))),\n",
    "    'top_terms': unique_topics,  # For ParlBERT, the \"top_terms\" is the topic label itself\n",
    "    'topic_label': unique_topics\n",
    "})\n",
    "\n",
    "print(f\"\\nTopics terms/labels dataframe shape: {df_topics_terms.shape}\")\n",
    "print(f\"\\nTopic mapping:\")\n",
    "print(df_topics_terms)\n",
    "\n",
    "# Create topic metrics dataframe (confidence scores per topic)\n",
    "df_topic_stats = df_topics_docs.group_by('dominant_topic').agg([\n",
    "    pl.col('dominant_topic_prob').mean().alias('mean_confidence'),\n",
    "    pl.col('dominant_topic_prob').std().alias('std_confidence'),\n",
    "    pl.len().alias('document_count')\n",
    "]).sort('dominant_topic')\n",
    "\n",
    "# Join with topic labels\n",
    "df_topic_metrics = df_topic_stats.join(\n",
    "    df_topics_terms.select(['topic', 'topic_label']),\n",
    "    left_on='dominant_topic',\n",
    "    right_on='topic',\n",
    "    how='left'\n",
    ").select(['dominant_topic', 'topic_label', 'mean_confidence', 'std_confidence', 'document_count'])\n",
    "df_topic_metrics = df_topic_metrics.rename({'dominant_topic': 'topic'})\n",
    "\n",
    "print(f\"\\nTopic metrics shape: {df_topic_metrics.shape}\")\n",
    "print(f\"\\nTopic metrics:\")\n",
    "print(df_topic_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72413d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ BERT Document-topic assignments saved (with all metadata):\n",
      "  Parquet: ..\\data\\processed\\topic_document_assignments_bert.parquet\n",
      "  CSV: ..\\data\\processed\\topic_document_assignments_bert.csv\n",
      "  Total columns: 18\n",
      "\n",
      "✓ BERT Topic terms/labels saved:\n",
      "  Parquet: ..\\data\\processed\\topic_terms_bert.parquet\n",
      "  CSV: ..\\data\\processed\\topic_terms_bert.csv\n",
      "\n",
      "✓ BERT Topic metrics saved:\n",
      "  Parquet: ..\\data\\processed\\topic_metrics_bert.parquet\n",
      "  CSV: ..\\data\\processed\\topic_metrics_bert.csv\n",
      "\n",
      "================================================================================\n",
      "PARLBERT TOPIC CLASSIFICATION RESULTS SAVED SUCCESSFULLY!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save all BERT results to parquet and csv\n",
    "output_dir = Path('../data/processed')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save document-topic assignments with all metadata (main result for merging)\n",
    "parquet_path_docs = output_dir / 'topic_document_assignments_bert.parquet'\n",
    "csv_path_docs = output_dir / 'topic_document_assignments_bert.csv'\n",
    "\n",
    "df_topics_docs.write_parquet(parquet_path_docs)\n",
    "df_topics_docs.write_csv(csv_path_docs)\n",
    "\n",
    "print(f\"\\n✓ BERT Document-topic assignments saved (with all metadata):\")\n",
    "print(f\"  Parquet: {parquet_path_docs}\")\n",
    "print(f\"  CSV: {csv_path_docs}\")\n",
    "print(f\"  Total columns: {df_topics_docs.width}\")\n",
    "\n",
    "# Save topic terms/labels\n",
    "parquet_path_terms = output_dir / 'topic_terms_bert.parquet'\n",
    "csv_path_terms = output_dir / 'topic_terms_bert.csv'\n",
    "\n",
    "df_topics_terms.write_parquet(parquet_path_terms)\n",
    "df_topics_terms.write_csv(csv_path_terms)\n",
    "\n",
    "print(f\"\\n✓ BERT Topic terms/labels saved:\")\n",
    "print(f\"  Parquet: {parquet_path_terms}\")\n",
    "print(f\"  CSV: {csv_path_terms}\")\n",
    "\n",
    "# Save topic metrics\n",
    "parquet_path_metrics = output_dir / 'topic_metrics_bert.parquet'\n",
    "csv_path_metrics = output_dir / 'topic_metrics_bert.csv'\n",
    "\n",
    "df_topic_metrics.write_parquet(parquet_path_metrics)\n",
    "df_topic_metrics.write_csv(csv_path_metrics)\n",
    "\n",
    "print(f\"\\n✓ BERT Topic metrics saved:\")\n",
    "print(f\"  Parquet: {parquet_path_metrics}\")\n",
    "print(f\"  CSV: {csv_path_metrics}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"PARLBERT TOPIC CLASSIFICATION RESULTS SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parl_speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
