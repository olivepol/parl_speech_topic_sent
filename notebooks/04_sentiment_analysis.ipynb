{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d92daef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LDA Topic Assignments loaded!\n",
      "  Shape: (588776, 17)\n",
      "  Topics: 15 unique topics\n",
      "\n",
      "Ready for sentiment analysis!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "# Load LDA topic assignments - this already has speechContent + topic info\n",
    "processed_dir = Path('../data/processed')\n",
    "df = pl.read_parquet(processed_dir / 'topic_document_assignments_lda.parquet')\n",
    "\n",
    "print(f\"âœ“ LDA Topic Assignments loaded!\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Topics: {df['dominant_topic'].n_unique()} unique topics\")\n",
    "print(f\"\\nReady for sentiment analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a6dc602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\olive\\miniconda3\\envs\\parl_speech\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'negative', 'positive', 'positive', 'neutral', 'neutral']\n",
      "['positive'] [[['positive', 0.9761366844177246], ['negative', 0.02354043908417225], ['neutral', 0.0003229434078093618]]]\n"
     ]
    }
   ],
   "source": [
    "# trying the sentiment API\n",
    "\n",
    "from germansentiment import SentimentModel\n",
    "\n",
    "model = SentimentModel()\n",
    "\n",
    "texts = [\n",
    "    \"Mit keinem guten Ergebniss\",\"Das ist gar nicht mal so gut\",\n",
    "    \"Total awesome!\",\"nicht so schlecht wie erwartet\",\n",
    "    \"Der Test verlief positiv.\",\"Sie fÃ¤hrt ein grÃ¼nes Auto.\"]\n",
    "       \n",
    "result = model.predict_sentiment(texts)\n",
    "print(result)\n",
    "\n",
    "classes, probabilities = model.predict_sentiment([\"das ist super\"], output_probabilities = True) \n",
    "print(classes, probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "430128a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimming paragraphs longer than 300 words...\n",
      "\n",
      "Trimming complete!\n",
      "Total rows: 588776\n",
      "\n",
      "Word count statistics after trimming:\n",
      "  Min words: 1\n",
      "  Max words: 300\n",
      "  Mean words: 29.8\n",
      "  Median words: 20.0\n",
      "\n",
      "First few rows after trimming:\n",
      "shape: (5, 3)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ id      â”† word_count â”† speechContent                   â”‚\n",
      "â”‚ ---     â”† ---        â”† ---                             â”‚\n",
      "â”‚ i64     â”† u32        â”† str                             â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 1000550 â”† 38         â”† Sehr geehrter Herr PrÃ¤sident Fâ€¦ â”‚\n",
      "â”‚ 1000550 â”† 1          â”† ({0})                           â”‚\n",
      "â”‚ 1000550 â”† 140        â”† Aber lassen Sie mich, bevor wiâ€¦ â”‚\n",
      "â”‚ 1000550 â”† 68         â”† Da komme ich zum SPD-Parteivorâ€¦ â”‚\n",
      "â”‚ 1000550 â”† 1          â”† ({1})                           â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# Cut paragraphs longer than 300 words using native Polars (10-100x faster than map_elements)\n",
    "import polars as pl\n",
    "from src.utils.text_processing import trim_to_max_words_native, add_word_count\n",
    "\n",
    "print(\"Trimming paragraphs longer than 300 words...\")\n",
    "\n",
    "# Apply trimming using native Polars operations (much faster than map_elements)\n",
    "df_trimmed = trim_to_max_words_native(df, 'speechContent', max_words=300)\n",
    "\n",
    "# Calculate word counts for statistics\n",
    "df_trimmed = add_word_count(df_trimmed, 'speechContent', 'word_count')\n",
    "\n",
    "print(f\"\\nTrimming complete!\")\n",
    "print(f\"Total rows: {df_trimmed.shape[0]}\")\n",
    "print(f\"\\nWord count statistics after trimming:\")\n",
    "print(f\"  Min words: {df_trimmed['word_count'].min()}\")\n",
    "print(f\"  Max words: {df_trimmed['word_count'].max()}\")\n",
    "print(f\"  Mean words: {df_trimmed['word_count'].mean():.1f}\")\n",
    "print(f\"  Median words: {df_trimmed['word_count'].median():.1f}\")\n",
    "\n",
    "print(f\"\\nFirst few rows after trimming:\")\n",
    "print(df_trimmed.select(['id', 'word_count', 'speechContent']).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df15434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SENTIMENT ANALYSIS - OPTIMIZED\n",
      "================================================================================\n",
      "Using 1% sample: 5,887 documents\n",
      "\n",
      "[1/4] Loading sentiment model...\n",
      "âœ“ Model loaded on: CPU\n",
      "\n",
      "[2/4] Configuration:\n",
      "  Total documents: 5,887\n",
      "  Batch size: 128\n",
      "  Total batches: 46\n",
      "  Checkpoint interval: every 50 batches\n",
      "  Estimated time: ~1.1 minutes\n",
      "\n",
      "[3/4] Running sentiment analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 24/46 [17:28<15:52, 43.29s/batch, docs=3,072/5,887, speed=3 docs/s, batch=44.74s, ETA=16.0min]"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis - OPTIMIZED VERSION with Status Updates\n",
    "from germansentiment import SentimentModel\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "import time\n",
    "import gc\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SENTIMENT ANALYSIS - OPTIMIZED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configuration - set to 1.0 for full dataset, 0.01 for 1% sample\n",
    "SAMPLE_FRACTION = 0.01  # Using 1% sample due to time constraints\n",
    "\n",
    "if SAMPLE_FRACTION < 1.0:\n",
    "    # Use stratified sampling to maintain topic/party distribution\n",
    "    df_analyze = df_trimmed.group_by(['dominant_topic', 'factionId']).sample(\n",
    "        fraction=SAMPLE_FRACTION, \n",
    "        seed=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "    print(f\"Using stratified {SAMPLE_FRACTION*100:.0f}% sample: {df_analyze.shape[0]:,} documents\")\n",
    "    print(f\"  Maintains distribution across topics and parties\")\n",
    "    \n",
    "    # Store the sampled IDs for later filtering of full dataset\n",
    "    sampled_ids = df_analyze.select(['id', 'paragraph_number']).unique()\n",
    "    print(f\"  Unique IDs in sample: {len(sampled_ids):,}\")\n",
    "else:\n",
    "    df_analyze = df_trimmed\n",
    "    sampled_ids = None\n",
    "    print(f\"Processing full dataset: {df_analyze.shape[0]:,} documents\")\n",
    "\n",
    "# Load model with GPU detection\n",
    "print(\"\\n[1/4] Loading sentiment model...\")\n",
    "model = SentimentModel()\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"âœ“ Model loaded on: {device.upper()}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    # Increase batch size for GPU\n",
    "    batch_size = 256  # GPU can handle larger batches\n",
    "else:\n",
    "    batch_size = 128  # CPU optimized batch size (doubled from 64)\n",
    "\n",
    "# Extract texts\n",
    "texts = df_analyze['speechContent'].to_list()\n",
    "total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "\n",
    "# Pre-allocate result lists for efficiency\n",
    "sentiments = [None] * len(texts)\n",
    "probabilities_list = [None] * len(texts)\n",
    "\n",
    "# Checkpoint configuration\n",
    "checkpoint_dir = Path('../data/interim')\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "checkpoint_file = checkpoint_dir / 'sentiment_checkpoint.json'\n",
    "checkpoint_interval = 50  # Save every 50 batches\n",
    "\n",
    "print(f\"\\n[2/4] Configuration:\")\n",
    "print(f\"  Total documents: {len(texts):,}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Total batches: {total_batches:,}\")\n",
    "print(f\"  Checkpoint interval: every {checkpoint_interval} batches\")\n",
    "print(f\"  Estimated time: ~{(total_batches * (1.5 if device == 'cpu' else 0.5)) / 60:.1f} minutes\")\n",
    "\n",
    "# Check for existing checkpoint\n",
    "start_batch = 0\n",
    "if checkpoint_file.exists():\n",
    "    print(f\"\\nâš ï¸  Found existing checkpoint!\")\n",
    "    response = input(\"Resume from checkpoint? (y/n): \")\n",
    "    if response.lower() == 'y':\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "        start_batch = checkpoint_data['last_batch'] + 1\n",
    "        sentiments[:checkpoint_data['processed_docs']] = checkpoint_data['sentiments']\n",
    "        probabilities_list[:checkpoint_data['processed_docs']] = checkpoint_data['probabilities']\n",
    "        print(f\"âœ“ Resuming from batch {start_batch} ({checkpoint_data['processed_docs']:,} docs already processed)\")\n",
    "\n",
    "# Process with progress bar\n",
    "print(f\"\\n[3/4] Running sentiment analysis...\")\n",
    "start_time = time.time()\n",
    "batch_times = []\n",
    "\n",
    "pbar = tqdm(range(start_batch, total_batches), desc=\"Processing\", unit=\"batch\", \n",
    "            initial=start_batch, total=total_batches)\n",
    "\n",
    "for batch_idx in pbar:\n",
    "    batch_start_time = time.time()\n",
    "    i = batch_idx * batch_size\n",
    "    batch_texts = [t if t else \"\" for t in texts[i:i+batch_size]]\n",
    "    \n",
    "    # Process batch\n",
    "    classes, probs = model.predict_sentiment(batch_texts, output_probabilities=True)\n",
    "    \n",
    "    # Store results efficiently\n",
    "    for j, (cls, prob) in enumerate(zip(classes, probs)):\n",
    "        idx = i + j\n",
    "        if idx < len(texts):\n",
    "            sentiments[idx] = cls\n",
    "            probabilities_list[idx] = prob\n",
    "    \n",
    "    # Track batch timing\n",
    "    batch_time = time.time() - batch_start_time\n",
    "    batch_times.append(batch_time)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    docs_done = min((batch_idx + 1) * batch_size, len(texts))\n",
    "    elapsed = time.time() - start_time\n",
    "    docs_per_sec = docs_done / elapsed if elapsed > 0 else 0\n",
    "    remaining_docs = len(texts) - docs_done\n",
    "    eta_seconds = remaining_docs / docs_per_sec if docs_per_sec > 0 else 0\n",
    "    \n",
    "    # Calculate average batch time (last 10 batches)\n",
    "    avg_batch_time = sum(batch_times[-10:]) / len(batch_times[-10:])\n",
    "    \n",
    "    # Enhanced progress display\n",
    "    pbar.set_postfix({\n",
    "        'docs': f'{docs_done:,}/{len(texts):,}',\n",
    "        'speed': f'{docs_per_sec:.0f} docs/s',\n",
    "        'batch': f'{batch_time:.2f}s',\n",
    "        'ETA': f'{eta_seconds/60:.1f}min'\n",
    "    })\n",
    "    \n",
    "    # Checkpoint saving\n",
    "    if (batch_idx + 1) % checkpoint_interval == 0:\n",
    "        checkpoint_data = {\n",
    "            'last_batch': batch_idx,\n",
    "            'processed_docs': docs_done,\n",
    "            'sentiments': sentiments[:docs_done],\n",
    "            'probabilities': [p.tolist() if hasattr(p, 'tolist') else p for p in probabilities_list[:docs_done]],\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        with open(checkpoint_file, 'w') as f:\n",
    "            json.dump(checkpoint_data, f)\n",
    "    \n",
    "    # Memory cleanup every 100 batches\n",
    "    if (batch_idx + 1) % 100 == 0:\n",
    "        gc.collect()\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "elapsed_total = time.time() - start_time\n",
    "\n",
    "# Remove checkpoint file on successful completion\n",
    "if checkpoint_file.exists():\n",
    "    checkpoint_file.unlink()\n",
    "\n",
    "# Add results to dataframe - store probabilities as JSON for efficiency\n",
    "print(f\"\\n[4/4] Creating results dataframe...\")\n",
    "df_sentiment = df_analyze.with_columns([\n",
    "    pl.Series('sentiment_class', sentiments),\n",
    "    pl.Series('sentiment_probabilities', [json.dumps(p.tolist() if hasattr(p, 'tolist') else p) for p in probabilities_list])\n",
    "])\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"SENTIMENT ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Total documents: {len(df_sentiment):,}\")\n",
    "print(f\"  Processing time: {elapsed_total/60:.1f} minutes\")\n",
    "print(f\"  Speed: {len(texts)/elapsed_total:.1f} docs/sec\")\n",
    "print(f\"  Avg batch time: {sum(batch_times)/len(batch_times):.2f}s\")\n",
    "print(f\"  Device used: {device.upper()}\")\n",
    "\n",
    "# Show sentiment distribution\n",
    "print(f\"\\nğŸ“Š Sentiment Distribution:\")\n",
    "sentiment_counts = df_sentiment['sentiment_class'].value_counts().sort('count', descending=True)\n",
    "for row in sentiment_counts.iter_rows(named=True):\n",
    "    percentage = (row['count'] / len(df_sentiment)) * 100\n",
    "    print(f\"  {row['sentiment_class']:10s}: {row['count']:6,} ({percentage:5.1f}%)\")\n",
    "\n",
    "# Cleanup\n",
    "del texts\n",
    "gc.collect()\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d9a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Sentiment analysis results saved!\n",
      "  File: ..\\data\\processed\\df_sample_sentiment.parquet\n",
      "  Rows: 412\n",
      "  Columns: 26\n",
      "\n",
      "ğŸ“Š Sentiment Analysis Summary:\n",
      "  Total paragraphs analyzed: 412\n",
      "\n",
      "  Sentiment Distribution:\n",
      "    neutral   : 261 ( 63.3%)\n",
      "    positive  : 107 ( 26.0%)\n",
      "    negative  :  44 ( 10.7%)\n",
      "\n",
      "âœ“ Data ready for further analysis!\n"
     ]
    }
   ],
   "source": [
    "# Save sentiment analysis results - ONLY matching IDs from sample\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING RESULTS - 1% SAMPLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define output path\n",
    "output_dir = Path('../data/processed')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# If we used sampling, filter the original full topic data to only include sampled IDs\n",
    "if SAMPLE_FRACTION < 1.0:\n",
    "    print(f\"\\nğŸ“Œ Filtering topic data to match sentiment sample IDs...\")\n",
    "    print(f\"  Original topic data: {df.shape[0]:,} rows\")\n",
    "    print(f\"  Sentiment sample: {df_sentiment.shape[0]:,} rows\")\n",
    "    \n",
    "    # Get the IDs from sentiment analysis sample\n",
    "    sample_ids = df_sentiment.select(['id', 'paragraph_number'])\n",
    "    \n",
    "    # Filter the original topic data to only include these IDs\n",
    "    df_filtered = df.join(\n",
    "        sample_ids,\n",
    "        on=['id', 'paragraph_number'],\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    print(f\"  Filtered topic data: {df_filtered.shape[0]:,} rows\")\n",
    "    print(f\"  âœ“ IDs match: {df_filtered.shape[0] == df_sentiment.shape[0]}\")\n",
    "    \n",
    "    # Now merge sentiment results with the filtered topic data\n",
    "    df_final = df_filtered.join(\n",
    "        df_sentiment.select(['id', 'paragraph_number', 'sentiment_class', 'sentiment_probabilities']),\n",
    "        on=['id', 'paragraph_number'],\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ“ Final merged data: {df_final.shape[0]:,} rows\")\n",
    "else:\n",
    "    # Full dataset - just merge directly\n",
    "    df_final = df_sentiment\n",
    "\n",
    "# Save the final dataset with topic + sentiment (1% sample)\n",
    "output_file = output_dir / 'df_sample_sentiment.parquet'\n",
    "df_final.write_parquet(output_file)\n",
    "\n",
    "print(f\"\\nâœ“ Sentiment analysis results saved!\")\n",
    "print(f\"  File: {output_file}\")\n",
    "print(f\"  Rows: {len(df_final):,}\")\n",
    "print(f\"  Columns: {len(df_final.columns)}\")\n",
    "print(f\"  Sample fraction: {SAMPLE_FRACTION*100:.0f}%\")\n",
    "\n",
    "# Show column list\n",
    "print(f\"\\nğŸ“‹ Columns in final dataset:\")\n",
    "for i, col in enumerate(df_final.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Show sample with topic + sentiment\n",
    "print(f\"\\nğŸ“Š Sample Results (Topic + Sentiment):\")\n",
    "print(df_final.select([\n",
    "    'id', 'firstName', 'lastName', 'factionId', \n",
    "    'dominant_topic', 'sentiment_class'\n",
    "]).head(10))\n",
    "\n",
    "# Cross-tabulation: Sentiment by Topic\n",
    "print(f\"\\nğŸ“Š Sentiment Distribution by Topic:\")\n",
    "cross_tab = df_final.group_by(['dominant_topic', 'sentiment_class']).len().sort(['dominant_topic', 'sentiment_class'])\n",
    "print(cross_tab.head(20))\n",
    "\n",
    "# Distribution by Party\n",
    "print(f\"\\nğŸ“Š Sentiment Distribution by Party:\")\n",
    "party_cross_tab = df_final.group_by(['factionId', 'sentiment_class']).len().sort(['factionId', 'sentiment_class'])\n",
    "print(party_cross_tab)\n",
    "\n",
    "print(f\"\\nâœ… Data ready for final analysis!\")\n",
    "print(f\"âš ï¸  Remember: This is a {SAMPLE_FRACTION*100:.0f}% stratified sample maintaining topic/party distributions\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parl_speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
