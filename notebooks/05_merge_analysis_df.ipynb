{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "885e85a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data files...\n",
      "✓ Loaded df_sample_sentiment.parquet: (3142, 18)\n",
      "✓ Loaded topic_document_assignments_bert.parquet: (412, 18)\n",
      "\n",
      "Dataframes loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "print(\"Loading data files...\")\n",
    "\n",
    "# Define paths\n",
    "processed_dir = Path('../data/processed')\n",
    "\n",
    "# Load sentiment data\n",
    "df_sentiment = pl.read_parquet(processed_dir / 'df_sample_sentiment.parquet')\n",
    "print(f\"✓ Loaded df_sample_sentiment.parquet: {df_sentiment.shape}\")\n",
    "\n",
    "# Load BERT topic data with document assignments (includes all metadata and topic_label)\n",
    "df_topic = pl.read_parquet(processed_dir / 'topic_document_assignments_bert.parquet')\n",
    "print(f\"✓ Loaded topic_document_assignments_bert.parquet: {df_topic.shape}\")\n",
    "\n",
    "print(f\"\\nDataframes loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b32bd5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING COMMON IDENTIFIERS\n",
      "================================================================================\n",
      "\n",
      "Sentiment dataframe columns:\n",
      "  ['id', 'session', 'electoralTerm', 'firstName', 'lastName', 'politicianId', 'speechContent', 'factionId', 'documentUrl', 'positionShort', 'positionLong', 'date', 'speech_length', 'paragraph_number', 'paragraph_length', 'word_count', 'sentiment_class', 'sentiment_probabilities']\n",
      "\n",
      "Topic dataframe columns:\n",
      "  ['id', 'session', 'electoralTerm', 'firstName', 'lastName', 'politicianId', 'factionId', 'documentUrl', 'positionShort', 'positionLong', 'date', 'speech_length', 'paragraph_number', 'paragraph_length', 'speechContent', 'dominant_topic', 'dominant_topic_prob', 'topic_label']\n",
      "\n",
      "1. Adding common ID to sentiment dataframe...\n",
      "✓ Created unique_id in sentiment data\n",
      "  Shape: (3142, 19)\n",
      "  Sample IDs: shape: (5, 1)\n",
      "┌───────────┐\n",
      "│ unique_id │\n",
      "│ ---       │\n",
      "│ str       │\n",
      "╞═══════════╡\n",
      "│ 739083_1  │\n",
      "│ 739083_2  │\n",
      "│ 739083_3  │\n",
      "│ 739083_4  │\n",
      "│ 739083_5  │\n",
      "└───────────┘\n",
      "\n",
      "2. Adding common ID to topic dataframe...\n",
      "✓ Created unique_id in topic data\n",
      "  Shape: (412, 19)\n",
      "  Sample IDs: shape: (5, 1)\n",
      "┌───────────┐\n",
      "│ unique_id │\n",
      "│ ---       │\n",
      "│ str       │\n",
      "╞═══════════╡\n",
      "│ 738998_1  │\n",
      "│ 738998_2  │\n",
      "│ 738998_3  │\n",
      "│ 738998_4  │\n",
      "│ 738998_5  │\n",
      "└───────────┘\n",
      "\n",
      "================================================================================\n",
      "COMMON IDENTIFIERS CREATED SUCCESSFULLY!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create common ID from speech ID and paragraph ID\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING COMMON IDENTIFIERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check columns in both dataframes\n",
    "print(\"\\nSentiment dataframe columns:\")\n",
    "print(f\"  {df_sentiment.columns}\")\n",
    "\n",
    "print(\"\\nTopic dataframe columns:\")\n",
    "print(f\"  {df_topic.columns}\")\n",
    "\n",
    "# Create common ID in sentiment dataframe\n",
    "print(\"\\n1. Adding common ID to sentiment dataframe...\")\n",
    "df_sentiment = df_sentiment.with_columns(\n",
    "    pl.concat_str(\n",
    "        pl.col('id').cast(pl.Utf8),\n",
    "        pl.lit('_'),\n",
    "        pl.col('paragraph_number').cast(pl.Utf8)\n",
    "    ).alias('unique_id')\n",
    ")\n",
    "print(f\"✓ Created unique_id in sentiment data\")\n",
    "print(f\"  Shape: {df_sentiment.shape}\")\n",
    "print(f\"  Sample IDs: {df_sentiment.select('unique_id').head(5)}\")\n",
    "\n",
    "# Create common ID in topic dataframe\n",
    "print(\"\\n2. Adding common ID to topic dataframe...\")\n",
    "df_topic = df_topic.with_columns(\n",
    "    pl.concat_str(\n",
    "        pl.col('id').cast(pl.Utf8),\n",
    "        pl.lit('_'),\n",
    "        pl.col('paragraph_number').cast(pl.Utf8)\n",
    "    ).alias('unique_id')\n",
    ")\n",
    "print(f\"✓ Created unique_id in topic data\")\n",
    "print(f\"  Shape: {df_topic.shape}\")\n",
    "print(f\"  Sample IDs: {df_topic.select('unique_id').head(5)}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"COMMON IDENTIFIERS CREATED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71165dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DEBUGGING JOIN ISSUE\n",
      "================================================================================\n",
      "\n",
      "Sentiment data:\n",
      "  Total rows: 3142\n",
      "  Unique IDs: 3142\n",
      "  Sample unique_ids:\n",
      "shape: (10, 1)\n",
      "┌───────────┐\n",
      "│ unique_id │\n",
      "│ ---       │\n",
      "│ str       │\n",
      "╞═══════════╡\n",
      "│ 739083_1  │\n",
      "│ 739083_2  │\n",
      "│ 739083_3  │\n",
      "│ 739083_4  │\n",
      "│ 739083_5  │\n",
      "│ 739083_6  │\n",
      "│ 739083_7  │\n",
      "│ 739083_8  │\n",
      "│ 739083_9  │\n",
      "│ 739083_10 │\n",
      "└───────────┘\n",
      "\n",
      "Topic data:\n",
      "  Total rows: 412\n",
      "  Unique IDs: 412\n",
      "  Sample unique_ids:\n",
      "shape: (10, 1)\n",
      "┌───────────┐\n",
      "│ unique_id │\n",
      "│ ---       │\n",
      "│ str       │\n",
      "╞═══════════╡\n",
      "│ 738998_1  │\n",
      "│ 738998_2  │\n",
      "│ 738998_3  │\n",
      "│ 738998_4  │\n",
      "│ 738998_5  │\n",
      "│ 738998_6  │\n",
      "│ 738998_7  │\n",
      "│ 738998_8  │\n",
      "│ 738998_9  │\n",
      "│ 738998_10 │\n",
      "└───────────┘\n",
      "\n",
      "*** OVERLAP ANALYSIS ***\n",
      "Sentiment unique IDs: 3142\n",
      "Topic unique IDs: 412\n",
      "Overlapping IDs: 3\n",
      "Overlapping IDs list: ['104406_1', '16838_1', '168418_1']\n",
      "\n",
      "*** WARNING: Very few overlapping IDs! ***\n",
      "\n",
      "First 5 sentiment IDs: ['1000419_1', '1000419_2', '1000419_3', '1000419_4', '1000419_5']\n",
      "First 5 topic IDs: ['1014536_1', '1014536_10', '1014536_11', '1014536_12', '1014536_13']\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check the unique IDs in both dataframes\n",
    "print(\"=\"*80)\n",
    "print(\"DEBUGGING JOIN ISSUE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nSentiment data:\")\n",
    "print(f\"  Total rows: {df_sentiment.shape[0]}\")\n",
    "print(f\"  Unique IDs: {df_sentiment['unique_id'].n_unique()}\")\n",
    "print(f\"  Sample unique_ids:\")\n",
    "print(df_sentiment.select('unique_id').head(10))\n",
    "\n",
    "print(f\"\\nTopic data:\")\n",
    "print(f\"  Total rows: {df_topic.shape[0]}\")\n",
    "print(f\"  Unique IDs: {df_topic['unique_id'].n_unique()}\")\n",
    "print(f\"  Sample unique_ids:\")\n",
    "print(df_topic.select('unique_id').head(10))\n",
    "\n",
    "# Check for overlapping IDs\n",
    "sentiment_ids = set(df_sentiment['unique_id'].to_list())\n",
    "topic_ids = set(df_topic['unique_id'].to_list())\n",
    "\n",
    "overlap = sentiment_ids & topic_ids\n",
    "print(f\"\\n*** OVERLAP ANALYSIS ***\")\n",
    "print(f\"Sentiment unique IDs: {len(sentiment_ids)}\")\n",
    "print(f\"Topic unique IDs: {len(topic_ids)}\")\n",
    "print(f\"Overlapping IDs: {len(overlap)}\")\n",
    "print(f\"Overlapping IDs list: {sorted(list(overlap))[:20]}\")  # Show first 20\n",
    "\n",
    "if len(overlap) < 10:\n",
    "    print(f\"\\n*** WARNING: Very few overlapping IDs! ***\")\n",
    "    print(f\"\\nFirst 5 sentiment IDs: {sorted(list(sentiment_ids))[:5]}\")\n",
    "    print(f\"First 5 topic IDs: {sorted(list(topic_ids))[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd54e168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING FINAL ANALYSIS DATAFRAME\n",
      "================================================================================\n",
      "\n",
      "1. Merging sentiment and BERT topic data...\n",
      "✓ Merged data shape: (3, 27)\n",
      "\n",
      "2. Creating final analysis dataframe...\n",
      "✓ Final dataframe created!\n",
      "  Shape: (3, 8)\n",
      "  Columns: ['time', 'speaker', 'party', 'speech', 'topic', 'topic_confidence', 'sentiment', 'unique_id']\n",
      "\n",
      "================================================================================\n",
      "FINAL DATAFRAME PREVIEW\n",
      "================================================================================\n",
      "shape: (3, 8)\n",
      "┌─────────────┬─────────────┬───────┬────────────┬────────────┬────────────┬───────────┬───────────┐\n",
      "│ time        ┆ speaker     ┆ party ┆ speech     ┆ topic      ┆ topic_conf ┆ sentiment ┆ unique_id │\n",
      "│ ---         ┆ ---         ┆ ---   ┆ ---        ┆ ---        ┆ idence     ┆ ---       ┆ ---       │\n",
      "│ str         ┆ str         ┆ i64   ┆ str        ┆ str        ┆ ---        ┆ str       ┆ str       │\n",
      "│             ┆             ┆       ┆            ┆            ┆ f64        ┆           ┆           │\n",
      "╞═════════════╪═════════════╪═══════╪════════════╪════════════╪════════════╪═══════════╪═══════════╡\n",
      "│ 1952-01-16T ┆ Pannenbecke ┆ 14    ┆ Herr       ┆ Government ┆ 0.999714   ┆ negative  ┆ 16838_1   │\n",
      "│ 00:00:00.00 ┆ r           ┆       ┆ Präsident! ┆            ┆            ┆           ┆           │\n",
      "│ 0000        ┆             ┆       ┆ Meine      ┆            ┆            ┆           ┆           │\n",
      "│             ┆             ┆       ┆ Damen un…  ┆            ┆            ┆           ┆           │\n",
      "│ 1964-11-05T ┆  schoettle  ┆ null  ┆ Haben Sie  ┆ Government ┆ 0.940278   ┆ neutral   ┆ 104406_1  │\n",
      "│ 00:00:00.00 ┆             ┆       ┆ noch eine  ┆            ┆            ┆           ┆           │\n",
      "│ 0000        ┆             ┆       ┆ Frage,     ┆            ┆            ┆           ┆           │\n",
      "│             ┆             ┆       ┆ Her…       ┆            ┆            ┆           ┆           │\n",
      "│ 1969-03-27T ┆  apel       ┆ 23    ┆ Das ist    ┆ Government ┆ 0.99966    ┆ neutral   ┆ 168418_1  │\n",
      "│ 00:00:00.00 ┆             ┆       ┆ richtig,   ┆            ┆            ┆           ┆           │\n",
      "│ 0000        ┆             ┆       ┆ Herr Dr.   ┆            ┆            ┆           ┆           │\n",
      "│             ┆             ┆       ┆ Wörn…      ┆            ┆            ┆           ┆           │\n",
      "└─────────────┴─────────────┴───────┴────────────┴────────────┴────────────┴───────────┴───────────┘\n",
      "\n",
      "================================================================================\n",
      "BERT TOPIC DISTRIBUTION\n",
      "================================================================================\n",
      "  Government                    :     3 speeches (100.00%)\n",
      "\n",
      "✓ Final dataframe saved:\n",
      "  Parquet: ..\\data\\processed\\df_final_analysis.parquet\n",
      "  CSV: ..\\data\\processed\\df_final_analysis.csv\n",
      "\n",
      "================================================================================\n",
      "FINAL ANALYSIS DATAFRAME CREATED SUCCESSFULLY!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create final analysis dataframe with BERT topics\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING FINAL ANALYSIS DATAFRAME\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Merge sentiment and topic data on unique_id\n",
    "print(\"\\n1. Merging sentiment and BERT topic data...\")\n",
    "df_merged = df_sentiment.join(\n",
    "    df_topic.select(['unique_id', 'dominant_topic', 'dominant_topic_prob', 'topic_label', \n",
    "                     'firstName', 'lastName', 'factionId', 'date', 'speechContent']),\n",
    "    on='unique_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"✓ Merged data shape: {df_merged.shape}\")\n",
    "\n",
    "# Create final dataframe with relevant columns\n",
    "print(\"\\n2. Creating final analysis dataframe...\")\n",
    "\n",
    "# Combine first and last name for speaker column\n",
    "df_final = df_merged.with_columns([\n",
    "    pl.concat_str([pl.col('firstName'), pl.lit(' '), pl.col('lastName')]).alias('speaker')\n",
    "])\n",
    "\n",
    "# Select and rename columns for final output\n",
    "df_final = df_final.select([\n",
    "    pl.col('date').alias('time'),\n",
    "    pl.col('speaker'),\n",
    "    pl.col('factionId').alias('party'),\n",
    "    pl.col('speechContent').alias('speech'),\n",
    "    pl.col('topic_label').alias('topic'),\n",
    "    pl.col('dominant_topic_prob').alias('topic_confidence'),\n",
    "    pl.col('sentiment_class').alias('sentiment'),\n",
    "    pl.col('unique_id')\n",
    "])\n",
    "\n",
    "print(f\"✓ Final dataframe created!\")\n",
    "print(f\"  Shape: {df_final.shape}\")\n",
    "print(f\"  Columns: {df_final.columns}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL DATAFRAME PREVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(df_final.head(10))\n",
    "\n",
    "# Show topic distribution\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"BERT TOPIC DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "topic_dist = df_final.group_by('topic').len().sort('len', descending=True)\n",
    "for row in topic_dist.iter_rows(named=True):\n",
    "    percentage = (row['len'] / df_final.shape[0]) * 100\n",
    "    print(f\"  {row['topic']:30s}: {row['len']:5d} speeches ({percentage:5.2f}%)\")\n",
    "\n",
    "# Save final dataframe\n",
    "output_dir = Path('../data/processed')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "parquet_path = output_dir / 'df_final_analysis.parquet'\n",
    "csv_path = output_dir / 'df_final_analysis.csv'\n",
    "\n",
    "df_final.write_parquet(parquet_path)\n",
    "df_final.write_csv(csv_path)\n",
    "\n",
    "print(f\"\\n✓ Final dataframe saved:\")\n",
    "print(f\"  Parquet: {parquet_path}\")\n",
    "print(f\"  CSV: {csv_path}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL ANALYSIS DATAFRAME CREATED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parl_speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
