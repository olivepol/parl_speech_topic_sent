{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "885e85a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data files...\n",
      "✓ Loaded df_sample_sentiment.parquet: (412, 18)\n",
      "✓ Loaded topic_document_assignments_bert.parquet: (412, 18)\n",
      "\n",
      "Dataframes loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "print(\"Loading data files...\")\n",
    "\n",
    "# Define paths\n",
    "processed_dir = Path('../data/processed')\n",
    "\n",
    "# Load sentiment data\n",
    "df_sentiment = pl.read_parquet(processed_dir / 'df_sample_sentiment.parquet')\n",
    "print(f\"✓ Loaded df_sample_sentiment.parquet: {df_sentiment.shape}\")\n",
    "\n",
    "# Load BERT topic data with document assignments (includes all metadata and topic_label)\n",
    "df_topic = pl.read_parquet(processed_dir / 'topic_document_assignments_bert.parquet')\n",
    "print(f\"✓ Loaded topic_document_assignments_bert.parquet: {df_topic.shape}\")\n",
    "\n",
    "print(f\"\\nDataframes loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b32bd5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING COMMON IDENTIFIERS\n",
      "================================================================================\n",
      "\n",
      "Sentiment dataframe columns:\n",
      "  ['id', 'session', 'electoralTerm', 'firstName', 'lastName', 'politicianId', 'speechContent', 'factionId', 'documentUrl', 'positionShort', 'positionLong', 'date', 'speech_length', 'paragraph_number', 'paragraph_length', 'word_count', 'sentiment_class', 'sentiment_probabilities']\n",
      "\n",
      "Topic dataframe columns:\n",
      "  ['id', 'session', 'electoralTerm', 'firstName', 'lastName', 'politicianId', 'factionId', 'documentUrl', 'positionShort', 'positionLong', 'date', 'speech_length', 'paragraph_number', 'paragraph_length', 'speechContent', 'dominant_topic', 'dominant_topic_prob', 'topic_label']\n",
      "\n",
      "1. Adding common ID to sentiment dataframe...\n",
      "✓ Created unique_id in sentiment data\n",
      "  Shape: (412, 19)\n",
      "  Sample IDs: shape: (5, 1)\n",
      "┌───────────┐\n",
      "│ unique_id │\n",
      "│ ---       │\n",
      "│ str       │\n",
      "╞═══════════╡\n",
      "│ 738998_1  │\n",
      "│ 738998_2  │\n",
      "│ 738998_3  │\n",
      "│ 738998_4  │\n",
      "│ 738998_5  │\n",
      "└───────────┘\n",
      "\n",
      "2. Adding common ID to topic dataframe...\n",
      "✓ Created unique_id in topic data\n",
      "  Shape: (412, 19)\n",
      "  Sample IDs: shape: (5, 1)\n",
      "┌───────────┐\n",
      "│ unique_id │\n",
      "│ ---       │\n",
      "│ str       │\n",
      "╞═══════════╡\n",
      "│ 738998_1  │\n",
      "│ 738998_2  │\n",
      "│ 738998_3  │\n",
      "│ 738998_4  │\n",
      "│ 738998_5  │\n",
      "└───────────┘\n",
      "\n",
      "================================================================================\n",
      "COMMON IDENTIFIERS CREATED SUCCESSFULLY!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create common ID from speech ID and paragraph ID\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING COMMON IDENTIFIERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check columns in both dataframes\n",
    "print(\"\\nSentiment dataframe columns:\")\n",
    "print(f\"  {df_sentiment.columns}\")\n",
    "\n",
    "print(\"\\nTopic dataframe columns:\")\n",
    "print(f\"  {df_topic.columns}\")\n",
    "\n",
    "# Create common ID in sentiment dataframe\n",
    "print(\"\\n1. Adding common ID to sentiment dataframe...\")\n",
    "df_sentiment = df_sentiment.with_columns(\n",
    "    pl.concat_str(\n",
    "        pl.col('id').cast(pl.Utf8),\n",
    "        pl.lit('_'),\n",
    "        pl.col('paragraph_number').cast(pl.Utf8)\n",
    "    ).alias('unique_id')\n",
    ")\n",
    "print(f\"✓ Created unique_id in sentiment data\")\n",
    "print(f\"  Shape: {df_sentiment.shape}\")\n",
    "print(f\"  Sample IDs: {df_sentiment.select('unique_id').head(5)}\")\n",
    "\n",
    "# Create common ID in topic dataframe\n",
    "print(\"\\n2. Adding common ID to topic dataframe...\")\n",
    "df_topic = df_topic.with_columns(\n",
    "    pl.concat_str(\n",
    "        pl.col('id').cast(pl.Utf8),\n",
    "        pl.lit('_'),\n",
    "        pl.col('paragraph_number').cast(pl.Utf8)\n",
    "    ).alias('unique_id')\n",
    ")\n",
    "print(f\"✓ Created unique_id in topic data\")\n",
    "print(f\"  Shape: {df_topic.shape}\")\n",
    "print(f\"  Sample IDs: {df_topic.select('unique_id').head(5)}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"COMMON IDENTIFIERS CREATED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd54e168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING FINAL ANALYSIS DATAFRAME\n",
      "================================================================================\n",
      "\n",
      "1. Merging sentiment and BERT topic data...\n",
      "✓ Merged data shape: (412, 27)\n",
      "\n",
      "2. Creating final analysis dataframe...\n",
      "✓ Final dataframe created!\n",
      "  Shape: (412, 8)\n",
      "  Columns: ['time', 'speaker', 'party', 'speech', 'topic', 'topic_confidence', 'sentiment', 'unique_id']\n",
      "\n",
      "================================================================================\n",
      "FINAL DATAFRAME PREVIEW\n",
      "================================================================================\n",
      "shape: (10, 8)\n",
      "┌─────────────┬─────────────┬───────┬────────────┬────────────┬────────────┬───────────┬───────────┐\n",
      "│ time        ┆ speaker     ┆ party ┆ speech     ┆ topic      ┆ topic_conf ┆ sentiment ┆ unique_id │\n",
      "│ ---         ┆ ---         ┆ ---   ┆ ---        ┆ ---        ┆ idence     ┆ ---       ┆ ---       │\n",
      "│ str         ┆ str         ┆ i64   ┆ str        ┆ str        ┆ ---        ┆ str       ┆ str       │\n",
      "│             ┆             ┆       ┆            ┆            ┆ f64        ┆           ┆           │\n",
      "╞═════════════╪═════════════╪═══════╪════════════╪════════════╪════════════╪═══════════╪═══════════╡\n",
      "│ 2010-05-20T ┆ burkhard    ┆ 23    ┆ Frau Präsi ┆ Government ┆ 0.942619   ┆ neutral   ┆ 738998_1  │\n",
      "│ 00:00:00.00 ┆ lischka     ┆       ┆ dentin!    ┆            ┆            ┆           ┆           │\n",
      "│ 0000        ┆             ┆       ┆ Meine      ┆            ┆            ┆           ┆           │\n",
      "│             ┆             ┆       ┆ Damen …    ┆            ┆            ┆           ┆           │\n",
      "│ 2010-05-20T ┆ burkhard    ┆ 23    ┆ Kollege    ┆ Government ┆ 0.990203   ┆ neutral   ┆ 738998_2  │\n",
      "│ 00:00:00.00 ┆ lischka     ┆       ┆ Heveling,  ┆            ┆            ┆           ┆           │\n",
      "│ 0000        ┆             ┆       ┆ ich bin    ┆            ┆            ┆           ┆           │\n",
      "│             ┆             ┆       ┆ auf …      ┆            ┆            ┆           ┆           │\n",
      "│ 2010-05-20T ┆ burkhard    ┆ 23    ┆ Der Anlass ┆ Law        ┆ 0.996563   ┆ neutral   ┆ 738998_3  │\n",
      "│ 00:00:00.00 ┆ lischka     ┆       ┆ für unsere ┆            ┆            ┆           ┆           │\n",
      "│ 0000        ┆             ┆       ┆ heutige …  ┆            ┆            ┆           ┆           │\n",
      "│ 2010-05-20T ┆ burkhard    ┆ 23    ┆ Zu den     ┆ Law        ┆ 0.998165   ┆ neutral   ┆ 738998_4  │\n",
      "│ 00:00:00.00 ┆ lischka     ┆       ┆ Zahlen.    ┆            ┆            ┆           ┆           │\n",
      "│ 0000        ┆             ┆       ┆ Sie        ┆            ┆            ┆           ┆           │\n",
      "│             ┆             ┆       ┆ wissen, 12 ┆            ┆            ┆           ┆           │\n",
      "│             ┆             ┆       ┆ …          ┆            ┆            ┆           ┆           │\n",
      "│ 2010-05-20T ┆ burkhard    ┆ 23    ┆ Allein die ┆ Law        ┆ 0.970507   ┆ neutral   ┆ 738998_5  │\n",
      "│ 00:00:00.00 ┆ lischka     ┆       ┆ Ermittlung ┆            ┆            ┆           ┆           │\n",
      "│ 0000        ┆             ┆       ┆ sstelle K… ┆            ┆            ┆           ┆           │\n",
      "│ 2010-05-20T ┆ burkhard    ┆ 23    ┆ Jetzt      ┆ Technology ┆ 0.606678   ┆ negative  ┆ 738998_6  │\n",
      "│ 00:00:00.00 ┆ lischka     ┆       ┆ liegt ein  ┆            ┆            ┆           ┆           │\n",
      "│ 0000        ┆             ┆       ┆ Richtlinie ┆            ┆            ┆           ┆           │\n",
      "│             ┆             ┆       ┆ nvor…      ┆            ┆            ┆           ┆           │\n",
      "│ 2010-05-20T ┆ burkhard    ┆ 23    ┆ Das erste  ┆ Technology ┆ 0.622835   ┆ negative  ┆ 738998_7  │\n",
      "│ 00:00:00.00 ┆ lischka     ┆       ┆ Thema ist  ┆            ┆            ┆           ┆           │\n",
      "│ 0000        ┆             ┆       ┆ schon      ┆            ┆            ┆           ┆           │\n",
      "│             ┆             ┆       ┆ ange…      ┆            ┆            ┆           ┆           │\n",
      "│ 2010-05-20T ┆ burkhard    ┆ 23    ┆ ({0})      ┆ Government ┆ 0.469457   ┆ neutral   ┆ 738998_8  │\n",
      "│ 00:00:00.00 ┆ lischka     ┆       ┆            ┆            ┆            ┆           ┆           │\n",
      "│ 0000        ┆             ┆       ┆            ┆            ┆            ┆           ┆           │\n",
      "│ 2010-05-20T ┆ burkhard    ┆ 23    ┆ Wir wollen ┆ Technology ┆ 0.668161   ┆ negative  ┆ 738998_9  │\n",
      "│ 00:00:00.00 ┆ lischka     ┆       ┆ Kinderporn ┆            ┆            ┆           ┆           │\n",
      "│ 0000        ┆             ┆       ┆ ografie i… ┆            ┆            ┆           ┆           │\n",
      "│ 2010-05-20T ┆ burkhard    ┆ 23    ┆ ({1})      ┆ Civil      ┆ 0.496788   ┆ positive  ┆ 738998_10 │\n",
      "│ 00:00:00.00 ┆ lischka     ┆       ┆            ┆            ┆            ┆           ┆           │\n",
      "│ 0000        ┆             ┆       ┆            ┆            ┆            ┆           ┆           │\n",
      "└─────────────┴─────────────┴───────┴────────────┴────────────┴────────────┴───────────┴───────────┘\n",
      "\n",
      "================================================================================\n",
      "BERT TOPIC DISTRIBUTION\n",
      "================================================================================\n",
      "  Government                    :   180 speeches (43.69%)\n",
      "  Civil                         :    86 speeches (20.87%)\n",
      "  International                 :    22 speeches ( 5.34%)\n",
      "  Law                           :    19 speeches ( 4.61%)\n",
      "  Macroeconomics                :    19 speeches ( 4.61%)\n",
      "  Labor                         :    16 speeches ( 3.88%)\n",
      "  Social                        :    15 speeches ( 3.64%)\n",
      "  Defense                       :    11 speeches ( 2.67%)\n",
      "  Agriculture                   :    10 speeches ( 2.43%)\n",
      "  Domestic                      :     8 speeches ( 1.94%)\n",
      "  Environment                   :     7 speeches ( 1.70%)\n",
      "  Technology                    :     5 speeches ( 1.21%)\n",
      "  Health                        :     4 speeches ( 0.97%)\n",
      "  Housing                       :     4 speeches ( 0.97%)\n",
      "  Energy                        :     2 speeches ( 0.49%)\n",
      "  Education                     :     2 speeches ( 0.49%)\n",
      "  Transportation                :     1 speeches ( 0.24%)\n",
      "  Foreign                       :     1 speeches ( 0.24%)\n",
      "\n",
      "✓ Final dataframe saved:\n",
      "  Parquet: ..\\data\\processed\\df_final_analysis.parquet\n",
      "  CSV: ..\\data\\processed\\df_final_analysis.csv\n",
      "\n",
      "================================================================================\n",
      "FINAL ANALYSIS DATAFRAME CREATED SUCCESSFULLY!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create final analysis dataframe with BERT topics\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING FINAL ANALYSIS DATAFRAME\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Merge sentiment and topic data on unique_id\n",
    "print(\"\\n1. Merging sentiment and BERT topic data...\")\n",
    "df_merged = df_sentiment.join(\n",
    "    df_topic.select(['unique_id', 'dominant_topic', 'dominant_topic_prob', 'topic_label', \n",
    "                     'firstName', 'lastName', 'factionId', 'date', 'speechContent']),\n",
    "    on='unique_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"✓ Merged data shape: {df_merged.shape}\")\n",
    "\n",
    "# Create final dataframe with relevant columns\n",
    "print(\"\\n2. Creating final analysis dataframe...\")\n",
    "\n",
    "# Combine first and last name for speaker column\n",
    "df_final = df_merged.with_columns([\n",
    "    pl.concat_str([pl.col('firstName'), pl.lit(' '), pl.col('lastName')]).alias('speaker')\n",
    "])\n",
    "\n",
    "# Select and rename columns for final output\n",
    "df_final = df_final.select([\n",
    "    pl.col('date').alias('time'),\n",
    "    pl.col('speaker'),\n",
    "    pl.col('factionId').alias('party'),\n",
    "    pl.col('speechContent').alias('speech'),\n",
    "    pl.col('topic_label').alias('topic'),\n",
    "    pl.col('dominant_topic_prob').alias('topic_confidence'),\n",
    "    pl.col('sentiment_class').alias('sentiment'),\n",
    "    pl.col('unique_id')\n",
    "])\n",
    "\n",
    "print(f\"✓ Final dataframe created!\")\n",
    "print(f\"  Shape: {df_final.shape}\")\n",
    "print(f\"  Columns: {df_final.columns}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL DATAFRAME PREVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(df_final.head(10))\n",
    "\n",
    "# Show topic distribution\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"BERT TOPIC DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "topic_dist = df_final.group_by('topic').len().sort('len', descending=True)\n",
    "for row in topic_dist.iter_rows(named=True):\n",
    "    percentage = (row['len'] / df_final.shape[0]) * 100\n",
    "    print(f\"  {row['topic']:30s}: {row['len']:5d} speeches ({percentage:5.2f}%)\")\n",
    "\n",
    "# Save final dataframe\n",
    "output_dir = Path('../data/processed')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "parquet_path = output_dir / 'df_final_analysis.parquet'\n",
    "csv_path = output_dir / 'df_final_analysis.csv'\n",
    "\n",
    "df_final.write_parquet(parquet_path)\n",
    "df_final.write_csv(csv_path)\n",
    "\n",
    "print(f\"\\n✓ Final dataframe saved:\")\n",
    "print(f\"  Parquet: {parquet_path}\")\n",
    "print(f\"  CSV: {csv_path}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL ANALYSIS DATAFRAME CREATED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parl_speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
