{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a5f9bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset loaded!\n",
      "Shape: (30688, 13)\n",
      "\n",
      "Column names: ['id', 'session', 'electoralTerm', 'firstName', 'lastName', 'politicianId', 'speechContent', 'factionId', 'documentUrl', 'positionShort', 'positionLong', 'date', 'speech_length']\n",
      "\n",
      "Data types:\n",
      "Schema([('id', Int64), ('session', Int64), ('electoralTerm', Int64), ('firstName', String), ('lastName', String), ('politicianId', Int64), ('speechContent', String), ('factionId', Int64), ('documentUrl', String), ('positionShort', String), ('positionLong', String), ('date', String), ('speech_length', Int64)])\n",
      "\n",
      "First few rows:\n",
      "shape: (5, 13)\n",
      "┌─────────┬─────────┬────────────┬───────────┬───┬────────────┬────────────┬───────────┬───────────┐\n",
      "│ id      ┆ session ┆ electoralT ┆ firstName ┆ … ┆ positionSh ┆ positionLo ┆ date      ┆ speech_le │\n",
      "│ ---     ┆ ---     ┆ erm        ┆ ---       ┆   ┆ ort        ┆ ng         ┆ ---       ┆ ngth      │\n",
      "│ i64     ┆ i64     ┆ ---        ┆ str       ┆   ┆ ---        ┆ ---        ┆ str       ┆ ---       │\n",
      "│         ┆         ┆ i64        ┆           ┆   ┆ str        ┆ str        ┆           ┆ i64       │\n",
      "╞═════════╪═════════╪════════════╪═══════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n",
      "│ 1000550 ┆ 4       ┆ 19         ┆ Florian   ┆ … ┆ Member of  ┆ NA         ┆ 2017-12-1 ┆ 4510      │\n",
      "│         ┆         ┆            ┆           ┆   ┆ Parliament ┆            ┆ 2T00:00:0 ┆           │\n",
      "│         ┆         ┆            ┆           ┆   ┆            ┆            ┆ 0.000000  ┆           │\n",
      "│ 1055788 ┆ 212     ┆ 19         ┆ Dietlind  ┆ … ┆ Member of  ┆ NA         ┆ 2021-02-2 ┆ 5394      │\n",
      "│         ┆         ┆            ┆           ┆   ┆ Parliament ┆            ┆ 5T00:00:0 ┆           │\n",
      "│         ┆         ┆            ┆           ┆   ┆            ┆            ┆ 0.000000  ┆           │\n",
      "│ 812191  ┆ 87      ┆ 18         ┆ Daniela   ┆ … ┆ Member of  ┆ NA         ┆ 2015-02-2 ┆ 1510      │\n",
      "│         ┆         ┆            ┆           ┆   ┆ Parliament ┆            ┆ 5T00:00:0 ┆           │\n",
      "│         ┆         ┆            ┆           ┆   ┆            ┆            ┆ 0.000000  ┆           │\n",
      "│ 840824  ┆ 219     ┆ 18         ┆ Carsten   ┆ … ┆ Member of  ┆ NA         ┆ 2017-02-1 ┆ 8367      │\n",
      "│         ┆         ┆            ┆           ┆   ┆ Parliament ┆            ┆ 7T00:00:0 ┆           │\n",
      "│         ┆         ┆            ┆           ┆   ┆            ┆            ┆ 0.000000  ┆           │\n",
      "│ 779072  ┆ 200     ┆ 17         ┆ Hubertus  ┆ … ┆ Member of  ┆ NA         ┆ 2012-10-2 ┆ 2643      │\n",
      "│         ┆         ┆            ┆           ┆   ┆ Parliament ┆            ┆ 4T00:00:0 ┆           │\n",
      "│         ┆         ┆            ┆           ┆   ┆            ┆            ┆ 0.000000  ┆           │\n",
      "└─────────┴─────────┴────────────┴───────────┴───┴────────────┴────────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from src.data.load_data import load_cleaned\n",
    "\n",
    "# Clear any cached imports\n",
    "if 'src.data.load_data' in sys.modules:\n",
    "    del sys.modules['src.data.load_data']\n",
    "    \n",
    "# Load the cleaned dataset\n",
    "df = load_cleaned()\n",
    "\n",
    "print(f\"Cleaned dataset loaded!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumn names: {df.columns}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.schema)\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60354733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARING PARAGRAPH SPLITTING METHODS\n",
      "================================================================================\n",
      "\n",
      "Method 1: Split by \\n\\n\n",
      "  Number of paragraphs: 17\n",
      "  Min length: 5\n",
      "  Max length: 1021\n",
      "  Avg length: 263\n",
      "\n",
      "Method 2: Split by \\n\n",
      "  Number of paragraphs: 17\n",
      "  Min length: 5\n",
      "  Max length: 1021\n",
      "  Avg length: 263\n",
      "\n",
      "Method 3: Split by regex (\\n\\s*\\n+)\n",
      "  Number of paragraphs: 17\n",
      "  Min length: 5\n",
      "  Max length: 1021\n",
      "  Avg length: 263\n",
      "\n",
      "================================================================================\n",
      "SAMPLE PARAGRAPHS (Method 1: \\n\\n)\n",
      "================================================================================\n",
      "\n",
      "Paragraph 1 (248 chars):\n",
      "Sehr geehrter Herr Präsident Friedrich! Kolleginnen und Kollegen! Die letzten zwei Sätze des Kollegen Hunko kann ich nur begrüßen. Da möchte ich mich ausdrücklich anschließen. Das war mal was Gescheit...\n",
      "\n",
      "Paragraph 2 (5 chars):\n",
      "({0})\n",
      "\n",
      "Paragraph 3 (1021 chars):\n",
      "Aber lassen Sie mich, bevor wir die einzelnen Themen des bevorstehenden Europäischen Rats besprechen, kurz noch grundsätzlich etwas zur europapolitischen Ausrichtung sagen. Für uns als CSU ist Europa ...\n"
     ]
    }
   ],
   "source": [
    "# Text Segmentation: Split speeches into paragraphs\n",
    "import polars as pl\n",
    "import re\n",
    "\n",
    "# Test different splitting methods on the first speech\n",
    "test_speech = df['speechContent'][0]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARING PARAGRAPH SPLITTING METHODS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Method 1: Split by double newline (\\n\\n)\n",
    "paragraphs_double_newline = test_speech.split('\\n\\n')\n",
    "paragraphs_double_newline = [p.strip() for p in paragraphs_double_newline if p.strip()]\n",
    "\n",
    "print(f\"\\nMethod 1: Split by \\\\n\\\\n\")\n",
    "print(f\"  Number of paragraphs: {len(paragraphs_double_newline)}\")\n",
    "print(f\"  Min length: {min(len(p) for p in paragraphs_double_newline) if paragraphs_double_newline else 0}\")\n",
    "print(f\"  Max length: {max(len(p) for p in paragraphs_double_newline) if paragraphs_double_newline else 0}\")\n",
    "print(f\"  Avg length: {sum(len(p) for p in paragraphs_double_newline) / len(paragraphs_double_newline) if paragraphs_double_newline else 0:.0f}\")\n",
    "\n",
    "# Method 2: Split by single newline (\\n)\n",
    "paragraphs_single_newline = test_speech.split('\\n')\n",
    "paragraphs_single_newline = [p.strip() for p in paragraphs_single_newline if p.strip()]\n",
    "\n",
    "print(f\"\\nMethod 2: Split by \\\\n\")\n",
    "print(f\"  Number of paragraphs: {len(paragraphs_single_newline)}\")\n",
    "print(f\"  Min length: {min(len(p) for p in paragraphs_single_newline) if paragraphs_single_newline else 0}\")\n",
    "print(f\"  Max length: {max(len(p) for p in paragraphs_single_newline) if paragraphs_single_newline else 0}\")\n",
    "print(f\"  Avg length: {sum(len(p) for p in paragraphs_single_newline) / len(paragraphs_single_newline) if paragraphs_single_newline else 0:.0f}\")\n",
    "\n",
    "# Method 3: Split by regex (one or more newlines/spaces)\n",
    "paragraphs_regex = re.split(r'\\n\\s*\\n+', test_speech)\n",
    "paragraphs_regex = [p.strip() for p in paragraphs_regex if p.strip()]\n",
    "\n",
    "print(f\"\\nMethod 3: Split by regex (\\\\n\\\\s*\\\\n+)\")\n",
    "print(f\"  Number of paragraphs: {len(paragraphs_regex)}\")\n",
    "print(f\"  Min length: {min(len(p) for p in paragraphs_regex) if paragraphs_regex else 0}\")\n",
    "print(f\"  Max length: {max(len(p) for p in paragraphs_regex) if paragraphs_regex else 0}\")\n",
    "print(f\"  Avg length: {sum(len(p) for p in paragraphs_regex) / len(paragraphs_regex) if paragraphs_regex else 0:.0f}\")\n",
    "\n",
    "# Sample paragraphs\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE PARAGRAPHS (Method 1: \\\\n\\\\n)\")\n",
    "print(\"=\" * 80)\n",
    "for i, para in enumerate(paragraphs_double_newline[:3]):\n",
    "    print(f\"\\nParagraph {i+1} ({len(para)} chars):\")\n",
    "    print(para[:200] + \"...\" if len(para) > 200 else para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89f80546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting speeches into paragraphs: 100%|██████████| 30688/30688 [00:00<00:00, 31450.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paragraph Segmentation Complete!\n",
      "\n",
      "Original dataset: 30688 speeches\n",
      "New dataset: 588776 paragraphs\n",
      "Average paragraphs per speech: 19.2\n",
      "\n",
      "New columns: ['id', 'session', 'electoralTerm', 'firstName', 'lastName', 'politicianId', 'speechContent', 'factionId', 'documentUrl', 'positionShort', 'positionLong', 'date', 'speech_length', 'paragraph_number', 'paragraph_length']\n",
      "\n",
      "Paragraph length statistics:\n",
      "  Min: 1\n",
      "  Max: 5527\n",
      "  Mean: 221\n",
      "  Median: 142\n",
      "\n",
      "First few rows:\n",
      "shape: (5, 15)\n",
      "┌─────────┬─────────┬────────────┬───────────┬───┬────────────┬────────────┬───────────┬───────────┐\n",
      "│ id      ┆ session ┆ electoralT ┆ firstName ┆ … ┆ date       ┆ speech_len ┆ paragraph ┆ paragraph │\n",
      "│ ---     ┆ ---     ┆ erm        ┆ ---       ┆   ┆ ---        ┆ gth        ┆ _number   ┆ _length   │\n",
      "│ i64     ┆ i64     ┆ ---        ┆ str       ┆   ┆ str        ┆ ---        ┆ ---       ┆ ---       │\n",
      "│         ┆         ┆ i64        ┆           ┆   ┆            ┆ i64        ┆ i64       ┆ i64       │\n",
      "╞═════════╪═════════╪════════════╪═══════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n",
      "│ 1000550 ┆ 4       ┆ 19         ┆ Florian   ┆ … ┆ 2017-12-12 ┆ 4510       ┆ 1         ┆ 248       │\n",
      "│         ┆         ┆            ┆           ┆   ┆ T00:00:00. ┆            ┆           ┆           │\n",
      "│         ┆         ┆            ┆           ┆   ┆ 000000     ┆            ┆           ┆           │\n",
      "│ 1000550 ┆ 4       ┆ 19         ┆ Florian   ┆ … ┆ 2017-12-12 ┆ 4510       ┆ 2         ┆ 5         │\n",
      "│         ┆         ┆            ┆           ┆   ┆ T00:00:00. ┆            ┆           ┆           │\n",
      "│         ┆         ┆            ┆           ┆   ┆ 000000     ┆            ┆           ┆           │\n",
      "│ 1000550 ┆ 4       ┆ 19         ┆ Florian   ┆ … ┆ 2017-12-12 ┆ 4510       ┆ 3         ┆ 1021      │\n",
      "│         ┆         ┆            ┆           ┆   ┆ T00:00:00. ┆            ┆           ┆           │\n",
      "│         ┆         ┆            ┆           ┆   ┆ 000000     ┆            ┆           ┆           │\n",
      "│ 1000550 ┆ 4       ┆ 19         ┆ Florian   ┆ … ┆ 2017-12-12 ┆ 4510       ┆ 4         ┆ 450       │\n",
      "│         ┆         ┆            ┆           ┆   ┆ T00:00:00. ┆            ┆           ┆           │\n",
      "│         ┆         ┆            ┆           ┆   ┆ 000000     ┆            ┆           ┆           │\n",
      "│ 1000550 ┆ 4       ┆ 19         ┆ Florian   ┆ … ┆ 2017-12-12 ┆ 4510       ┆ 5         ┆ 5         │\n",
      "│         ┆         ┆            ┆           ┆   ┆ T00:00:00. ┆            ┆           ┆           │\n",
      "│         ┆         ┆            ┆           ┆   ┆ 000000     ┆            ┆           ┆           │\n",
      "└─────────┴─────────┴────────────┴───────────┴───┴────────────┴────────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# Apply Method 1 (split by \\n\\n) to all speeches\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "def split_speech_into_paragraphs(speech_content: str) -> list:\n",
    "    \"\"\"Split speech into paragraphs by double newline.\"\"\"\n",
    "    paragraphs = speech_content.split('\\n\\n')\n",
    "    paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "    return paragraphs\n",
    "\n",
    "# Create a new dataframe with paragraphs as separate rows\n",
    "rows_list = []\n",
    "\n",
    "for row in tqdm(df.iter_rows(named=True), total=df.shape[0], desc=\"Splitting speeches into paragraphs\"):\n",
    "    speech_content = row['speechContent']\n",
    "    paragraphs = split_speech_into_paragraphs(speech_content)\n",
    "    \n",
    "    # Create a row for each paragraph\n",
    "    for para_num, paragraph in enumerate(paragraphs, 1):\n",
    "        new_row = {\n",
    "            **row,  # Copy all metadata from original speech\n",
    "            'speechContent': paragraph,  # Replace with paragraph\n",
    "            'paragraph_number': para_num,  # Add paragraph number\n",
    "            'paragraph_length': len(paragraph)  # Add paragraph length\n",
    "        }\n",
    "        rows_list.append(new_row)\n",
    "\n",
    "# Create new dataframe with paragraphs\n",
    "df_paragraphs = pl.DataFrame(rows_list)\n",
    "\n",
    "print(\"\\nParagraph Segmentation Complete!\")\n",
    "print(f\"\\nOriginal dataset: {df.shape[0]} speeches\")\n",
    "print(f\"New dataset: {df_paragraphs.shape[0]} paragraphs\")\n",
    "print(f\"Average paragraphs per speech: {df_paragraphs.shape[0] / df.shape[0]:.1f}\")\n",
    "\n",
    "print(f\"\\nNew columns: {df_paragraphs.columns}\")\n",
    "print(f\"\\nParagraph length statistics:\")\n",
    "print(f\"  Min: {df_paragraphs['paragraph_length'].min()}\")\n",
    "print(f\"  Max: {df_paragraphs['paragraph_length'].max()}\")\n",
    "print(f\"  Mean: {df_paragraphs['paragraph_length'].mean():.0f}\")\n",
    "print(f\"  Median: {df_paragraphs['paragraph_length'].median():.0f}\")\n",
    "\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df_paragraphs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93321930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented paragraphs saved!\n",
      "Output file: ..\\data\\interim\\df_sample_split.csv\n",
      "File size: 218219.05 KB\n",
      "Total rows (paragraphs): 588776\n",
      "Total columns: 15\n"
     ]
    }
   ],
   "source": [
    "# Save segmented paragraphs to interim folder\n",
    "from pathlib import Path\n",
    "\n",
    "# Define output path\n",
    "interim_dir = Path('../data/interim')\n",
    "interim_dir.mkdir(exist_ok=True)\n",
    "\n",
    "output_file = interim_dir / 'df_sample_split.csv'\n",
    "\n",
    "# Save as CSV\n",
    "df_paragraphs.write_csv(output_file)\n",
    "\n",
    "print(f\"Segmented paragraphs saved!\")\n",
    "print(f\"Output file: {output_file}\")\n",
    "print(f\"File size: {output_file.stat().st_size / 1024:.2f} KB\")\n",
    "print(f\"Total rows (paragraphs): {df_paragraphs.shape[0]}\")\n",
    "print(f\"Total columns: {df_paragraphs.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c420dd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercase conversion complete!\n",
      "\n",
      "All 588776 paragraphs converted to lowercase\n",
      "\n",
      "Sample (first paragraph, first 200 chars):\n",
      "sehr geehrter herr präsident friedrich! kolleginnen und kollegen! die letzten zwei sätze des kollegen hunko kann ich nur begrüßen. da möchte ich mich ausdrücklich anschließen. das war mal was gescheit\n"
     ]
    }
   ],
   "source": [
    "# Convert all speeches to lowercase\n",
    "import polars as pl\n",
    "\n",
    "# Convert speechContent to lowercase\n",
    "df_paragraphs_lowercase = df_paragraphs.with_columns(\n",
    "    pl.col('speechContent').str.to_lowercase().alias('speechContent')\n",
    ")\n",
    "\n",
    "print(\"Lowercase conversion complete!\")\n",
    "print(f\"\\nAll {df_paragraphs_lowercase.shape[0]} paragraphs converted to lowercase\")\n",
    "print(f\"\\nSample (first paragraph, first 200 chars):\")\n",
    "print(df_paragraphs_lowercase['speechContent'][0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d798045e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy model 'de_core_news_sm' loaded successfully!\n",
      "Active pipeline components: ['tok2vec', 'morphologizer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing paragraphs: 100%|██████████| 588776/588776 [20:36<00:00, 475.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization complete!\n",
      "\n",
      "Dataframe shape: (588776, 17)\n",
      "New columns: ['id', 'session', 'electoralTerm', 'firstName', 'lastName', 'politicianId', 'speechContent', 'factionId', 'documentUrl', 'positionShort', 'positionLong', 'date', 'speech_length', 'paragraph_number', 'paragraph_length', 'tokens', 'token_count']\n",
      "\n",
      "Token count statistics:\n",
      "  Min: 1\n",
      "  Max: 885\n",
      "  Mean: 39.6\n",
      "  Median: 25\n",
      "\n",
      "Sample (first paragraph tokens):\n",
      "  Tokens: shape: (44,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Sehr\"\n",
      "\t\"geehrter\"\n",
      "\t\"Herr\"\n",
      "\t\"Präsident\"\n",
      "\t\"Friedrich\"\n",
      "\t…\n",
      "\t\"Ich\"\n",
      "\t\"bin\"\n",
      "\t\"ganz\"\n",
      "\t\"begeistert\"\n",
      "\t\".\"\n",
      "]\n",
      "  Token count: 44\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all paragraphs using spaCy (OPTIMIZED with nlp.pipe)\n",
    "import spacy\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load German language model\n",
    "try:\n",
    "    nlp = spacy.load('de_core_news_sm')\n",
    "    print(\"spaCy model 'de_core_news_sm' loaded successfully!\")\n",
    "except OSError:\n",
    "    print(\"Model not found. Installing de_core_news_sm...\")\n",
    "    import subprocess\n",
    "    subprocess.run(['python', '-m', 'spacy', 'download', 'de_core_news_sm'])\n",
    "    nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "# OPTIMIZATION: Disable unused pipeline components for faster tokenization\n",
    "# We only need the tokenizer at this stage\n",
    "nlp.disable_pipes(['tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer'])\n",
    "print(f\"Active pipeline components: {nlp.pipe_names}\")\n",
    "\n",
    "# Get texts as list\n",
    "texts = df_paragraphs['speechContent'].to_list()\n",
    "\n",
    "# OPTIMIZATION: Use nlp.pipe() for batch processing (10-100x faster than individual nlp() calls)\n",
    "# batch_size controls memory vs speed tradeoff\n",
    "tokens_list = []\n",
    "for doc in tqdm(nlp.pipe(texts, batch_size=100), total=len(texts), desc=\"Tokenizing paragraphs\"):\n",
    "    tokens = [token.text for token in doc]\n",
    "    tokens_list.append(tokens)\n",
    "\n",
    "# Re-enable all components for later use\n",
    "nlp.enable_pipe('tagger')\n",
    "nlp.enable_pipe('parser')\n",
    "nlp.enable_pipe('ner')\n",
    "nlp.enable_pipe('attribute_ruler')\n",
    "nlp.enable_pipe('lemmatizer')\n",
    "\n",
    "# Add tokens column to dataframe\n",
    "df_paragraphs = df_paragraphs.with_columns(\n",
    "    pl.Series('tokens', tokens_list)\n",
    ")\n",
    "\n",
    "# Add token count column\n",
    "df_paragraphs = df_paragraphs.with_columns(\n",
    "    pl.col('tokens').list.len().alias('token_count')\n",
    ")\n",
    "\n",
    "print(\"\\nTokenization complete!\")\n",
    "print(f\"\\nDataframe shape: {df_paragraphs.shape}\")\n",
    "print(f\"New columns: {df_paragraphs.columns}\")\n",
    "\n",
    "print(f\"\\nToken count statistics:\")\n",
    "print(f\"  Min: {df_paragraphs['token_count'].min()}\")\n",
    "print(f\"  Max: {df_paragraphs['token_count'].max()}\")\n",
    "print(f\"  Mean: {df_paragraphs['token_count'].mean():.1f}\")\n",
    "print(f\"  Median: {df_paragraphs['token_count'].median():.0f}\")\n",
    "\n",
    "print(f\"\\nSample (first paragraph tokens):\")\n",
    "print(f\"  Tokens: {df_paragraphs['tokens'][0]}\")\n",
    "print(f\"  Token count: {df_paragraphs['token_count'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66854068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopword Removal (Memory Optimized)\n",
      "Number of German stopwords: 543\n",
      "Sample stopwords: ['wenige', 'am', 'weil', 'dürfen', 'statt', 'seines', 'dank', 'vom', 'eigenes', 'damals', 'wenigstens', 'zuerst', 'zwanzig', 'neben', 'manchen', 'würden', 'gehabt', 'jemanden', 'ab', 'drin']\n",
      "  Processed 50000/588776 paragraphs...\n",
      "  Processed 100000/588776 paragraphs...\n",
      "  Processed 150000/588776 paragraphs...\n",
      "  Processed 200000/588776 paragraphs...\n",
      "  Processed 250000/588776 paragraphs...\n",
      "  Processed 300000/588776 paragraphs...\n",
      "  Processed 350000/588776 paragraphs...\n",
      "  Processed 400000/588776 paragraphs...\n",
      "  Processed 450000/588776 paragraphs...\n",
      "  Processed 500000/588776 paragraphs...\n",
      "  Processed 550000/588776 paragraphs...\n",
      "  Processed 588776/588776 paragraphs...\n",
      "\n",
      "Stopword removal complete!\n",
      "\n",
      "Dataframe shape: (588776, 19)\n",
      "\n",
      "Token count comparison:\n",
      "  Original tokens - Mean: 39.6\n",
      "  After stopword removal - Mean: 21.3\n",
      "  Reduction: 46.2%\n",
      "\n",
      "Sample comparison (first paragraph):\n",
      "  Original tokens (44): shape: (44,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Sehr\"\n",
      "\t\"geehrter\"\n",
      "\t\"Herr\"\n",
      "\t\"Präsident\"\n",
      "\t\"Friedrich\"\n",
      "\t…\n",
      "\t\"Ich\"\n",
      "\t\"bin\"\n",
      "\t\"ganz\"\n",
      "\t\"begeistert\"\n",
      "\t\".\"\n",
      "]\n",
      "  After stopword removal (24): shape: (24,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"geehrter\"\n",
      "\t\"Herr\"\n",
      "\t\"Präsident\"\n",
      "\t\"Friedrich\"\n",
      "\t\"!\"\n",
      "\t…\n",
      "\t\"linken\"\n",
      "\t\"Ecke\"\n",
      "\t\".\"\n",
      "\t\"begeistert\"\n",
      "\t\".\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords from tokens (MEMORY OPTIMIZED)\n",
    "import polars as pl\n",
    "import gc\n",
    "\n",
    "# Use stopwords from the already-loaded nlp model (don't reload!)\n",
    "# nlp should already be in memory from the tokenization cell\n",
    "german_stopwords = set(nlp.Defaults.stop_words)  # Use set for O(1) lookup\n",
    "\n",
    "print(\"Stopword Removal (Memory Optimized)\")\n",
    "print(f\"Number of German stopwords: {len(german_stopwords)}\")\n",
    "print(f\"Sample stopwords: {list(german_stopwords)[:20]}\")\n",
    "\n",
    "# Process in batches to avoid memory issues\n",
    "tokens_list = df_paragraphs['tokens'].to_list()\n",
    "tokens_no_stopwords_list = []\n",
    "\n",
    "batch_size = 10000\n",
    "for i in range(0, len(tokens_list), batch_size):\n",
    "    batch = tokens_list[i:i+batch_size]\n",
    "    for tokens in batch:\n",
    "        filtered = [token for token in tokens if token.lower() not in german_stopwords]\n",
    "        tokens_no_stopwords_list.append(filtered)\n",
    "    \n",
    "    # Progress indicator\n",
    "    if (i + batch_size) % 50000 == 0 or i + batch_size >= len(tokens_list):\n",
    "        print(f\"  Processed {min(i + batch_size, len(tokens_list))}/{len(tokens_list)} paragraphs...\")\n",
    "\n",
    "# Clear the original list to free memory\n",
    "del tokens_list\n",
    "gc.collect()\n",
    "\n",
    "# Add new columns\n",
    "df_paragraphs = df_paragraphs.with_columns([\n",
    "    pl.Series('tokens_no_stopwords', tokens_no_stopwords_list),\n",
    "])\n",
    "\n",
    "# Add count column separately to avoid memory spike\n",
    "df_paragraphs = df_paragraphs.with_columns(\n",
    "    pl.col('tokens_no_stopwords').list.len().alias('token_count_no_stopwords')\n",
    ")\n",
    "\n",
    "# Clean up\n",
    "del tokens_no_stopwords_list\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nStopword removal complete!\")\n",
    "print(f\"\\nDataframe shape: {df_paragraphs.shape}\")\n",
    "\n",
    "print(f\"\\nToken count comparison:\")\n",
    "print(f\"  Original tokens - Mean: {df_paragraphs['token_count'].mean():.1f}\")\n",
    "print(f\"  After stopword removal - Mean: {df_paragraphs['token_count_no_stopwords'].mean():.1f}\")\n",
    "print(f\"  Reduction: {(1 - df_paragraphs['token_count_no_stopwords'].mean() / df_paragraphs['token_count'].mean()) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nSample comparison (first paragraph):\")\n",
    "print(f\"  Original tokens ({df_paragraphs['token_count'][0]}): {df_paragraphs['tokens'][0]}\")\n",
    "print(f\"  After stopword removal ({df_paragraphs['token_count_no_stopwords'][0]}): {df_paragraphs['tokens_no_stopwords'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3f0b8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation and Number Removal (Memory Optimized)\n",
      "Punctuation characters: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "  Processed 50000/588776 paragraphs...\n",
      "  Processed 100000/588776 paragraphs...\n",
      "  Processed 150000/588776 paragraphs...\n",
      "  Processed 200000/588776 paragraphs...\n",
      "  Processed 250000/588776 paragraphs...\n",
      "  Processed 300000/588776 paragraphs...\n",
      "  Processed 350000/588776 paragraphs...\n",
      "  Processed 400000/588776 paragraphs...\n",
      "  Processed 450000/588776 paragraphs...\n",
      "  Processed 500000/588776 paragraphs...\n",
      "  Processed 550000/588776 paragraphs...\n",
      "  Processed 588776/588776 paragraphs...\n",
      "\n",
      "Punctuation and number removal complete!\n",
      "Dataframe shape: (588776, 21)\n",
      "\n",
      "Token count progression:\n",
      "  Original tokens - Mean: 39.6\n",
      "  After stopword removal - Mean: 21.3\n",
      "  After punct/number removal - Mean: 12.5\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation and numbers from tokens (MEMORY OPTIMIZED)\n",
    "import polars as pl\n",
    "import string\n",
    "import re\n",
    "import gc\n",
    "\n",
    "# Don't reload spaCy - it's already in memory!\n",
    "\n",
    "print(\"Punctuation and Number Removal (Memory Optimized)\")\n",
    "print(f\"Punctuation characters: {string.punctuation}\")\n",
    "\n",
    "# Pre-compile regex for speed\n",
    "punct_num_pattern = re.compile(r'[\\d\\W]')\n",
    "punct_digits_set = set(string.punctuation + string.digits)\n",
    "\n",
    "# Process in batches\n",
    "tokens_list = df_paragraphs['tokens_no_stopwords'].to_list()\n",
    "tokens_clean_list = []\n",
    "\n",
    "batch_size = 10000\n",
    "for i in range(0, len(tokens_list), batch_size):\n",
    "    batch = tokens_list[i:i+batch_size]\n",
    "    for tokens in batch:\n",
    "        cleaned = []\n",
    "        for token in tokens:\n",
    "            if all(c in punct_digits_set for c in token):\n",
    "                continue\n",
    "            cleaned_token = punct_num_pattern.sub('', token)\n",
    "            if cleaned_token:\n",
    "                cleaned.append(cleaned_token)\n",
    "        tokens_clean_list.append(cleaned)\n",
    "    \n",
    "    if (i + batch_size) % 50000 == 0 or i + batch_size >= len(tokens_list):\n",
    "        print(f\"  Processed {min(i + batch_size, len(tokens_list))}/{len(tokens_list)} paragraphs...\")\n",
    "\n",
    "del tokens_list\n",
    "gc.collect()\n",
    "\n",
    "# Add columns\n",
    "df_paragraphs = df_paragraphs.with_columns(\n",
    "    pl.Series('tokens_clean', tokens_clean_list)\n",
    ")\n",
    "df_paragraphs = df_paragraphs.with_columns(\n",
    "    pl.col('tokens_clean').list.len().alias('token_count_clean')\n",
    ")\n",
    "\n",
    "del tokens_clean_list\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nPunctuation and number removal complete!\")\n",
    "print(f\"Dataframe shape: {df_paragraphs.shape}\")\n",
    "\n",
    "print(f\"\\nToken count progression:\")\n",
    "print(f\"  Original tokens - Mean: {df_paragraphs['token_count'].mean():.1f}\")\n",
    "print(f\"  After stopword removal - Mean: {df_paragraphs['token_count_no_stopwords'].mean():.1f}\")\n",
    "print(f\"  After punct/number removal - Mean: {df_paragraphs['token_count_clean'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d93ac0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization (Memory Optimized)\n",
      "Converting tokens to their base/lemma form...\n",
      "Active pipeline components: ['tok2vec', 'tagger', 'morphologizer', 'lemmatizer', 'attribute_ruler']\n",
      "  Processed 5000/588776 paragraphs...\n",
      "  Processed 10000/588776 paragraphs...\n",
      "  Processed 15000/588776 paragraphs...\n",
      "  Processed 20000/588776 paragraphs...\n",
      "  Processed 25000/588776 paragraphs...\n",
      "  Processed 30000/588776 paragraphs...\n",
      "  Processed 35000/588776 paragraphs...\n",
      "  Processed 40000/588776 paragraphs...\n",
      "  Processed 45000/588776 paragraphs...\n",
      "  Processed 50000/588776 paragraphs...\n",
      "  Processed 55000/588776 paragraphs...\n",
      "  Processed 60000/588776 paragraphs...\n",
      "  Processed 65000/588776 paragraphs...\n",
      "  Processed 70000/588776 paragraphs...\n",
      "  Processed 75000/588776 paragraphs...\n",
      "  Processed 80000/588776 paragraphs...\n",
      "  Processed 85000/588776 paragraphs...\n",
      "  Processed 90000/588776 paragraphs...\n",
      "  Processed 95000/588776 paragraphs...\n",
      "  Processed 100000/588776 paragraphs...\n",
      "  Processed 105000/588776 paragraphs...\n",
      "  Processed 110000/588776 paragraphs...\n",
      "  Processed 115000/588776 paragraphs...\n",
      "  Processed 120000/588776 paragraphs...\n",
      "  Processed 125000/588776 paragraphs...\n",
      "  Processed 130000/588776 paragraphs...\n",
      "  Processed 135000/588776 paragraphs...\n",
      "  Processed 140000/588776 paragraphs...\n",
      "  Processed 145000/588776 paragraphs...\n",
      "  Processed 150000/588776 paragraphs...\n",
      "  Processed 155000/588776 paragraphs...\n",
      "  Processed 160000/588776 paragraphs...\n",
      "  Processed 165000/588776 paragraphs...\n",
      "  Processed 170000/588776 paragraphs...\n",
      "  Processed 175000/588776 paragraphs...\n",
      "  Processed 180000/588776 paragraphs...\n",
      "  Processed 185000/588776 paragraphs...\n",
      "  Processed 190000/588776 paragraphs...\n",
      "  Processed 195000/588776 paragraphs...\n",
      "  Processed 200000/588776 paragraphs...\n",
      "  Processed 205000/588776 paragraphs...\n",
      "  Processed 210000/588776 paragraphs...\n",
      "  Processed 215000/588776 paragraphs...\n",
      "  Processed 220000/588776 paragraphs...\n",
      "  Processed 225000/588776 paragraphs...\n",
      "  Processed 230000/588776 paragraphs...\n",
      "  Processed 235000/588776 paragraphs...\n",
      "  Processed 240000/588776 paragraphs...\n",
      "  Processed 245000/588776 paragraphs...\n",
      "  Processed 250000/588776 paragraphs...\n",
      "  Processed 255000/588776 paragraphs...\n",
      "  Processed 260000/588776 paragraphs...\n",
      "  Processed 265000/588776 paragraphs...\n",
      "  Processed 270000/588776 paragraphs...\n",
      "  Processed 275000/588776 paragraphs...\n",
      "  Processed 280000/588776 paragraphs...\n",
      "  Processed 285000/588776 paragraphs...\n",
      "  Processed 290000/588776 paragraphs...\n",
      "  Processed 295000/588776 paragraphs...\n",
      "  Processed 300000/588776 paragraphs...\n",
      "  Processed 305000/588776 paragraphs...\n",
      "  Processed 310000/588776 paragraphs...\n",
      "  Processed 315000/588776 paragraphs...\n",
      "  Processed 320000/588776 paragraphs...\n",
      "  Processed 325000/588776 paragraphs...\n",
      "  Processed 330000/588776 paragraphs...\n",
      "  Processed 335000/588776 paragraphs...\n",
      "  Processed 340000/588776 paragraphs...\n",
      "  Processed 345000/588776 paragraphs...\n",
      "  Processed 350000/588776 paragraphs...\n",
      "  Processed 355000/588776 paragraphs...\n",
      "  Processed 360000/588776 paragraphs...\n",
      "  Processed 365000/588776 paragraphs...\n",
      "  Processed 370000/588776 paragraphs...\n",
      "  Processed 375000/588776 paragraphs...\n",
      "  Processed 380000/588776 paragraphs...\n",
      "  Processed 385000/588776 paragraphs...\n",
      "  Processed 390000/588776 paragraphs...\n",
      "  Processed 395000/588776 paragraphs...\n",
      "  Processed 400000/588776 paragraphs...\n",
      "  Processed 405000/588776 paragraphs...\n",
      "  Processed 410000/588776 paragraphs...\n",
      "  Processed 415000/588776 paragraphs...\n",
      "  Processed 420000/588776 paragraphs...\n",
      "  Processed 425000/588776 paragraphs...\n",
      "  Processed 430000/588776 paragraphs...\n",
      "  Processed 435000/588776 paragraphs...\n",
      "  Processed 440000/588776 paragraphs...\n",
      "  Processed 445000/588776 paragraphs...\n",
      "  Processed 450000/588776 paragraphs...\n",
      "  Processed 455000/588776 paragraphs...\n",
      "  Processed 460000/588776 paragraphs...\n",
      "  Processed 465000/588776 paragraphs...\n",
      "  Processed 470000/588776 paragraphs...\n",
      "  Processed 475000/588776 paragraphs...\n",
      "  Processed 480000/588776 paragraphs...\n",
      "  Processed 485000/588776 paragraphs...\n",
      "  Processed 490000/588776 paragraphs...\n",
      "  Processed 495000/588776 paragraphs...\n",
      "  Processed 500000/588776 paragraphs...\n",
      "  Processed 505000/588776 paragraphs...\n",
      "  Processed 510000/588776 paragraphs...\n",
      "  Processed 515000/588776 paragraphs...\n",
      "  Processed 520000/588776 paragraphs...\n",
      "  Processed 525000/588776 paragraphs...\n",
      "  Processed 530000/588776 paragraphs...\n",
      "  Processed 535000/588776 paragraphs...\n",
      "  Processed 540000/588776 paragraphs...\n",
      "  Processed 545000/588776 paragraphs...\n",
      "  Processed 550000/588776 paragraphs...\n",
      "  Processed 555000/588776 paragraphs...\n",
      "  Processed 560000/588776 paragraphs...\n",
      "  Processed 565000/588776 paragraphs...\n",
      "  Processed 570000/588776 paragraphs...\n",
      "  Processed 575000/588776 paragraphs...\n",
      "  Processed 580000/588776 paragraphs...\n",
      "  Processed 585000/588776 paragraphs...\n",
      "  Processed 588776/588776 paragraphs...\n",
      "\n",
      "Lemmatization complete!\n",
      "Dataframe shape: (588776, 23)\n",
      "\n",
      "Token count at each stage:\n",
      "  Original: 39.6\n",
      "  After stopwords: 21.3\n",
      "  After punct/num: 12.5\n",
      "  After lemma: 12.5\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization (MEMORY OPTIMIZED - smaller batches)\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Don't reload spaCy! Reuse nlp from tokenization cell\n",
    "\n",
    "print(\"Lemmatization (Memory Optimized)\")\n",
    "print(\"Converting tokens to their base/lemma form...\")\n",
    "\n",
    "# Disable components we don't need\n",
    "nlp.disable_pipes(['parser', 'ner'])\n",
    "print(f\"Active pipeline components: {nlp.pipe_names}\")\n",
    "\n",
    "# Process in smaller batches to avoid OOM\n",
    "tokens_clean = df_paragraphs['tokens_clean'].to_list()\n",
    "tokens_lemma_list = []\n",
    "\n",
    "# Use smaller batch size for memory efficiency\n",
    "batch_size = 5000\n",
    "nlp_batch_size = 50  # Smaller nlp.pipe batch\n",
    "\n",
    "for i in range(0, len(tokens_clean), batch_size):\n",
    "    batch = tokens_clean[i:i+batch_size]\n",
    "    texts_batch = [' '.join(tokens) if tokens else '' for tokens in batch]\n",
    "    \n",
    "    for doc in nlp.pipe(texts_batch, batch_size=nlp_batch_size):\n",
    "        lemmas = [token.lemma_ for token in doc]\n",
    "        tokens_lemma_list.append(lemmas)\n",
    "    \n",
    "    # Free memory after each batch\n",
    "    del texts_batch\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"  Processed {min(i + batch_size, len(tokens_clean))}/{len(tokens_clean)} paragraphs...\")\n",
    "\n",
    "del tokens_clean\n",
    "gc.collect()\n",
    "\n",
    "# Re-enable components\n",
    "nlp.enable_pipe('parser')\n",
    "nlp.enable_pipe('ner')\n",
    "\n",
    "# Add to dataframe\n",
    "df_paragraphs = df_paragraphs.with_columns(\n",
    "    pl.Series('tokens_lemma', tokens_lemma_list)\n",
    ")\n",
    "df_paragraphs = df_paragraphs.with_columns(\n",
    "    pl.col('tokens_lemma').list.len().alias('token_count_lemma')\n",
    ")\n",
    "\n",
    "del tokens_lemma_list\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nLemmatization complete!\")\n",
    "print(f\"Dataframe shape: {df_paragraphs.shape}\")\n",
    "\n",
    "print(f\"\\nToken count at each stage:\")\n",
    "print(f\"  Original: {df_paragraphs['token_count'].mean():.1f}\")\n",
    "print(f\"  After stopwords: {df_paragraphs['token_count_no_stopwords'].mean():.1f}\")\n",
    "print(f\"  After punct/num: {df_paragraphs['token_count_clean'].mean():.1f}\")\n",
    "print(f\"  After lemma: {df_paragraphs['token_count_lemma'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9be6e99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved!\n",
      "Output file: ..\\data\\processed\\df_sample_split_preprocessed.parquet\n",
      "File size: 204685.89 KB\n",
      "Total rows (paragraphs): 588776\n",
      "Total columns: 23\n",
      "\n",
      "Dataframe columns:\n",
      "  - id\n",
      "  - session\n",
      "  - electoralTerm\n",
      "  - firstName\n",
      "  - lastName\n",
      "  - politicianId\n",
      "  - speechContent\n",
      "  - factionId\n",
      "  - documentUrl\n",
      "  - positionShort\n",
      "  - positionLong\n",
      "  - date\n",
      "  - speech_length\n",
      "  - paragraph_number\n",
      "  - paragraph_length\n",
      "  - tokens\n",
      "  - token_count\n",
      "  - tokens_no_stopwords\n",
      "  - token_count_no_stopwords\n",
      "  - tokens_clean\n",
      "  - token_count_clean\n",
      "  - tokens_lemma\n",
      "  - token_count_lemma\n"
     ]
    }
   ],
   "source": [
    "# Save preprocessed data to processed folder\n",
    "from pathlib import Path\n",
    "\n",
    "# Define output path\n",
    "processed_dir = Path('../data/processed')\n",
    "processed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save as Parquet (supports nested data like lists)\n",
    "output_file = processed_dir / 'df_sample_split_preprocessed.parquet'\n",
    "df_paragraphs.write_parquet(output_file)\n",
    "\n",
    "print(f\"Preprocessed data saved!\")\n",
    "print(f\"Output file: {output_file}\")\n",
    "print(f\"File size: {output_file.stat().st_size / 1024:.2f} KB\")\n",
    "print(f\"Total rows (paragraphs): {df_paragraphs.shape[0]}\")\n",
    "print(f\"Total columns: {df_paragraphs.shape[1]}\")\n",
    "print(f\"\\nDataframe columns:\")\n",
    "for col in df_paragraphs.columns:\n",
    "    print(f\"  - {col}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parl_speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
