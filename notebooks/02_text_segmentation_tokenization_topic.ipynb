{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a5f9bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset loaded!\n",
      "Shape: (563, 13)\n",
      "\n",
      "Column names: ['id', 'session', 'electoralTerm', 'firstName', 'lastName', 'politicianId', 'speechContent', 'factionId', 'documentUrl', 'positionShort', 'positionLong', 'date', 'speech_length']\n",
      "\n",
      "Data types:\n",
      "Schema([('id', Int64), ('session', Int64), ('electoralTerm', Int64), ('firstName', String), ('lastName', String), ('politicianId', Int64), ('speechContent', String), ('factionId', Int64), ('documentUrl', String), ('positionShort', String), ('positionLong', String), ('date', String), ('speech_length', Int64)])\n",
      "\n",
      "First few rows:\n",
      "shape: (5, 13)\n",
      "┌─────────┬─────────┬────────────┬───────────┬───┬────────────┬────────────┬───────────┬───────────┐\n",
      "│ id      ┆ session ┆ electoralT ┆ firstName ┆ … ┆ positionSh ┆ positionLo ┆ date      ┆ speech_le │\n",
      "│ ---     ┆ ---     ┆ erm        ┆ ---       ┆   ┆ ort        ┆ ng         ┆ ---       ┆ ngth      │\n",
      "│ i64     ┆ i64     ┆ ---        ┆ str       ┆   ┆ ---        ┆ ---        ┆ str       ┆ ---       │\n",
      "│         ┆         ┆ i64        ┆           ┆   ┆ str        ┆ str        ┆           ┆ i64       │\n",
      "╞═════════╪═════════╪════════════╪═══════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n",
      "│ 739083  ┆ 43      ┆ 17         ┆ lukrezia  ┆ … ┆ Member of  ┆ NA         ┆ 2010-05-2 ┆ 3564      │\n",
      "│         ┆         ┆            ┆           ┆   ┆ Parliament ┆            ┆ 0T00:00:0 ┆           │\n",
      "│         ┆         ┆            ┆           ┆   ┆            ┆            ┆ 0.000000  ┆           │\n",
      "│ 1046338 ┆ 179     ┆ 19         ┆ Tobias    ┆ … ┆ Member of  ┆ NA         ┆ 2020-09-3 ┆ 5534      │\n",
      "│         ┆         ┆            ┆           ┆   ┆ Parliament ┆            ┆ 0T00:00:0 ┆           │\n",
      "│         ┆         ┆            ┆           ┆   ┆            ┆            ┆ 0.000000  ┆           │\n",
      "│ 636323  ┆ 229     ┆ 14         ┆ Gudrun    ┆ … ┆ Secretary  ┆ parl. staa ┆ 2002-04-1 ┆ 1595      │\n",
      "│         ┆         ┆            ┆           ┆   ┆ of State   ┆ tssekretär ┆ 7T00:00:0 ┆           │\n",
      "│         ┆         ┆            ┆           ┆   ┆            ┆ in bei     ┆ 0.000000  ┆           │\n",
      "│         ┆         ┆            ┆           ┆   ┆            ┆ der…       ┆           ┆           │\n",
      "│ 533719  ┆ 21      ┆ 13         ┆ Uwe       ┆ … ┆ Member of  ┆ NA         ┆ 1995-02-1 ┆ 313       │\n",
      "│         ┆         ┆            ┆           ┆   ┆ Parliament ┆            ┆ 6T00:00:0 ┆           │\n",
      "│         ┆         ┆            ┆           ┆   ┆            ┆            ┆ 0.000000  ┆           │\n",
      "│ 549177  ┆ 83      ┆ 13         ┆ Burkhard  ┆ … ┆ Presidium  ┆ vizepräsid ┆ 1996-02-0 ┆ 2454      │\n",
      "│         ┆         ┆            ┆           ┆   ┆ of         ┆ ent        ┆ 1T00:00:0 ┆           │\n",
      "│         ┆         ┆            ┆           ┆   ┆ Parliament ┆            ┆ 0.000000  ┆           │\n",
      "└─────────┴─────────┴────────────┴───────────┴───┴────────────┴────────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from src.data.load_data import load_cleaned\n",
    "\n",
    "# Clear any cached imports\n",
    "if 'src.data.load_data' in sys.modules:\n",
    "    del sys.modules['src.data.load_data']\n",
    "    \n",
    "# Load the cleaned dataset\n",
    "df = load_cleaned()\n",
    "\n",
    "print(f\"Cleaned dataset loaded!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumn names: {df.columns}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.schema)\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60354733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARING PARAGRAPH SPLITTING METHODS\n",
      "================================================================================\n",
      "\n",
      "Method 1: Split by \\n\\n\n",
      "  Number of paragraphs: 29\n",
      "  Min length: 5\n",
      "  Max length: 520\n",
      "  Avg length: 121\n",
      "\n",
      "Method 2: Split by \\n\n",
      "  Number of paragraphs: 65\n",
      "  Min length: 5\n",
      "  Max length: 208\n",
      "  Avg length: 53\n",
      "\n",
      "Method 3: Split by regex (\\n\\s*\\n+)\n",
      "  Number of paragraphs: 29\n",
      "  Min length: 5\n",
      "  Max length: 520\n",
      "  Avg length: 121\n",
      "\n",
      "================================================================================\n",
      "SAMPLE PARAGRAPHS (Method 1: \\n\\n)\n",
      "================================================================================\n",
      "\n",
      "Paragraph 1 (51 chars):\n",
      "Herr Präsident! Liebe Kolleginnen und Kollegen! Vor\n",
      "\n",
      "Paragraph 2 (520 chars):\n",
      "total leeren Tribünen - mit Ausnahme der Eltern des\n",
      "Kollegen Kurth, die dankenswerterweise da sind - diskutieren wir jetzt als letzten Tagesordnungspunkt über\n",
      "eines der großen Themen der Erinnerungsku...\n",
      "\n",
      "Paragraph 3 (5 chars):\n",
      "({0})\n"
     ]
    }
   ],
   "source": [
    "# Text Segmentation: Split speeches into paragraphs\n",
    "import polars as pl\n",
    "import re\n",
    "\n",
    "# Test different splitting methods on the first speech\n",
    "test_speech = df['speechContent'][0]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARING PARAGRAPH SPLITTING METHODS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Method 1: Split by double newline (\\n\\n)\n",
    "paragraphs_double_newline = test_speech.split('\\n\\n')\n",
    "paragraphs_double_newline = [p.strip() for p in paragraphs_double_newline if p.strip()]\n",
    "\n",
    "print(f\"\\nMethod 1: Split by \\\\n\\\\n\")\n",
    "print(f\"  Number of paragraphs: {len(paragraphs_double_newline)}\")\n",
    "print(f\"  Min length: {min(len(p) for p in paragraphs_double_newline) if paragraphs_double_newline else 0}\")\n",
    "print(f\"  Max length: {max(len(p) for p in paragraphs_double_newline) if paragraphs_double_newline else 0}\")\n",
    "print(f\"  Avg length: {sum(len(p) for p in paragraphs_double_newline) / len(paragraphs_double_newline) if paragraphs_double_newline else 0:.0f}\")\n",
    "\n",
    "# Method 2: Split by single newline (\\n)\n",
    "paragraphs_single_newline = test_speech.split('\\n')\n",
    "paragraphs_single_newline = [p.strip() for p in paragraphs_single_newline if p.strip()]\n",
    "\n",
    "print(f\"\\nMethod 2: Split by \\\\n\")\n",
    "print(f\"  Number of paragraphs: {len(paragraphs_single_newline)}\")\n",
    "print(f\"  Min length: {min(len(p) for p in paragraphs_single_newline) if paragraphs_single_newline else 0}\")\n",
    "print(f\"  Max length: {max(len(p) for p in paragraphs_single_newline) if paragraphs_single_newline else 0}\")\n",
    "print(f\"  Avg length: {sum(len(p) for p in paragraphs_single_newline) / len(paragraphs_single_newline) if paragraphs_single_newline else 0:.0f}\")\n",
    "\n",
    "# Method 3: Split by regex (one or more newlines/spaces)\n",
    "paragraphs_regex = re.split(r'\\n\\s*\\n+', test_speech)\n",
    "paragraphs_regex = [p.strip() for p in paragraphs_regex if p.strip()]\n",
    "\n",
    "print(f\"\\nMethod 3: Split by regex (\\\\n\\\\s*\\\\n+)\")\n",
    "print(f\"  Number of paragraphs: {len(paragraphs_regex)}\")\n",
    "print(f\"  Min length: {min(len(p) for p in paragraphs_regex) if paragraphs_regex else 0}\")\n",
    "print(f\"  Max length: {max(len(p) for p in paragraphs_regex) if paragraphs_regex else 0}\")\n",
    "print(f\"  Avg length: {sum(len(p) for p in paragraphs_regex) / len(paragraphs_regex) if paragraphs_regex else 0:.0f}\")\n",
    "\n",
    "# Sample paragraphs\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE PARAGRAPHS (Method 1: \\\\n\\\\n)\")\n",
    "print(\"=\" * 80)\n",
    "for i, para in enumerate(paragraphs_double_newline[:3]):\n",
    "    print(f\"\\nParagraph {i+1} ({len(para)} chars):\")\n",
    "    print(para[:200] + \"...\" if len(para) > 200 else para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89f80546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph Segmentation Complete!\n",
      "\n",
      "Original dataset: 563 speeches\n",
      "New dataset: 3142 paragraphs\n",
      "Average paragraphs per speech: 5.6\n",
      "\n",
      "New columns: ['id', 'session', 'electoralTerm', 'firstName', 'lastName', 'politicianId', 'speechContent', 'factionId', 'documentUrl', 'positionShort', 'positionLong', 'date', 'speech_length', 'paragraph_number', 'paragraph_length']\n",
      "\n",
      "Paragraph length statistics:\n",
      "  Min: 5\n",
      "  Max: 25375\n",
      "  Mean: 529\n",
      "  Median: 197\n",
      "\n",
      "First few rows:\n",
      "shape: (5, 15)\n",
      "┌────────┬─────────┬────────────┬───────────┬───┬────────────┬────────────┬────────────┬───────────┐\n",
      "│ id     ┆ session ┆ electoralT ┆ firstName ┆ … ┆ date       ┆ speech_len ┆ paragraph_ ┆ paragraph │\n",
      "│ ---    ┆ ---     ┆ erm        ┆ ---       ┆   ┆ ---        ┆ gth        ┆ number     ┆ _length   │\n",
      "│ i64    ┆ i64     ┆ ---        ┆ str       ┆   ┆ str        ┆ ---        ┆ ---        ┆ ---       │\n",
      "│        ┆         ┆ i64        ┆           ┆   ┆            ┆ i64        ┆ i64        ┆ i64       │\n",
      "╞════════╪═════════╪════════════╪═══════════╪═══╪════════════╪════════════╪════════════╪═══════════╡\n",
      "│ 739083 ┆ 43      ┆ 17         ┆ lukrezia  ┆ … ┆ 2010-05-20 ┆ 3564       ┆ 1          ┆ 51        │\n",
      "│        ┆         ┆            ┆           ┆   ┆ T00:00:00. ┆            ┆            ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆ 000000     ┆            ┆            ┆           │\n",
      "│ 739083 ┆ 43      ┆ 17         ┆ lukrezia  ┆ … ┆ 2010-05-20 ┆ 3564       ┆ 2          ┆ 520       │\n",
      "│        ┆         ┆            ┆           ┆   ┆ T00:00:00. ┆            ┆            ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆ 000000     ┆            ┆            ┆           │\n",
      "│ 739083 ┆ 43      ┆ 17         ┆ lukrezia  ┆ … ┆ 2010-05-20 ┆ 3564       ┆ 3          ┆ 5         │\n",
      "│        ┆         ┆            ┆           ┆   ┆ T00:00:00. ┆            ┆            ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆ 000000     ┆            ┆            ┆           │\n",
      "│ 739083 ┆ 43      ┆ 17         ┆ lukrezia  ┆ … ┆ 2010-05-20 ┆ 3564       ┆ 4          ┆ 48        │\n",
      "│        ┆         ┆            ┆           ┆   ┆ T00:00:00. ┆            ┆            ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆ 000000     ┆            ┆            ┆           │\n",
      "│ 739083 ┆ 43      ┆ 17         ┆ lukrezia  ┆ … ┆ 2010-05-20 ┆ 3564       ┆ 5          ┆ 5         │\n",
      "│        ┆         ┆            ┆           ┆   ┆ T00:00:00. ┆            ┆            ┆           │\n",
      "│        ┆         ┆            ┆           ┆   ┆ 000000     ┆            ┆            ┆           │\n",
      "└────────┴─────────┴────────────┴───────────┴───┴────────────┴────────────┴────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# Apply Method 1 (split by \\n\\n) to all speeches\n",
    "import polars as pl\n",
    "\n",
    "def split_speech_into_paragraphs(speech_content: str) -> list:\n",
    "    \"\"\"Split speech into paragraphs by double newline.\"\"\"\n",
    "    paragraphs = speech_content.split('\\n\\n')\n",
    "    paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "    return paragraphs\n",
    "\n",
    "# Create a new dataframe with paragraphs as separate rows\n",
    "rows_list = []\n",
    "\n",
    "for row in df.iter_rows(named=True):\n",
    "    speech_content = row['speechContent']\n",
    "    paragraphs = split_speech_into_paragraphs(speech_content)\n",
    "    \n",
    "    # Create a row for each paragraph\n",
    "    for para_num, paragraph in enumerate(paragraphs, 1):\n",
    "        new_row = {\n",
    "            **row,  # Copy all metadata from original speech\n",
    "            'speechContent': paragraph,  # Replace with paragraph\n",
    "            'paragraph_number': para_num,  # Add paragraph number\n",
    "            'paragraph_length': len(paragraph)  # Add paragraph length\n",
    "        }\n",
    "        rows_list.append(new_row)\n",
    "\n",
    "# Create new dataframe with paragraphs\n",
    "df_paragraphs = pl.DataFrame(rows_list)\n",
    "\n",
    "print(\"Paragraph Segmentation Complete!\")\n",
    "print(f\"\\nOriginal dataset: {df.shape[0]} speeches\")\n",
    "print(f\"New dataset: {df_paragraphs.shape[0]} paragraphs\")\n",
    "print(f\"Average paragraphs per speech: {df_paragraphs.shape[0] / df.shape[0]:.1f}\")\n",
    "\n",
    "print(f\"\\nNew columns: {df_paragraphs.columns}\")\n",
    "print(f\"\\nParagraph length statistics:\")\n",
    "print(f\"  Min: {df_paragraphs['paragraph_length'].min()}\")\n",
    "print(f\"  Max: {df_paragraphs['paragraph_length'].max()}\")\n",
    "print(f\"  Mean: {df_paragraphs['paragraph_length'].mean():.0f}\")\n",
    "print(f\"  Median: {df_paragraphs['paragraph_length'].median():.0f}\")\n",
    "\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df_paragraphs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93321930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented paragraphs saved!\n",
      "Output file: ..\\data\\interim\\df_sample_split.csv\n",
      "File size: 2144.53 KB\n",
      "Total rows (paragraphs): 3142\n",
      "Total columns: 15\n"
     ]
    }
   ],
   "source": [
    "# Save segmented paragraphs to interim folder\n",
    "from pathlib import Path\n",
    "\n",
    "# Define output path\n",
    "interim_dir = Path('../data/interim')\n",
    "interim_dir.mkdir(exist_ok=True)\n",
    "\n",
    "output_file = interim_dir / 'df_sample_split.csv'\n",
    "\n",
    "# Save as CSV\n",
    "df_paragraphs.write_csv(output_file)\n",
    "\n",
    "print(f\"Segmented paragraphs saved!\")\n",
    "print(f\"Output file: {output_file}\")\n",
    "print(f\"File size: {output_file.stat().st_size / 1024:.2f} KB\")\n",
    "print(f\"Total rows (paragraphs): {df_paragraphs.shape[0]}\")\n",
    "print(f\"Total columns: {df_paragraphs.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c420dd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercase conversion complete!\n",
      "\n",
      "All 3142 paragraphs converted to lowercase\n",
      "\n",
      "Sample (first paragraph, first 200 chars):\n",
      "herr präsident! liebe kolleginnen und kollegen! vor\n"
     ]
    }
   ],
   "source": [
    "# Convert all speeches to lowercase\n",
    "import polars as pl\n",
    "\n",
    "# Convert speechContent to lowercase\n",
    "df_paragraphs_lowercase = df_paragraphs.with_columns(\n",
    "    pl.col('speechContent').str.to_lowercase().alias('speechContent')\n",
    ")\n",
    "\n",
    "print(\"Lowercase conversion complete!\")\n",
    "print(f\"\\nAll {df_paragraphs_lowercase.shape[0]} paragraphs converted to lowercase\")\n",
    "print(f\"\\nSample (first paragraph, first 200 chars):\")\n",
    "print(df_paragraphs_lowercase['speechContent'][0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d798045e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy model 'de_core_news_sm' loaded successfully!\n",
      "Tokenization complete!\n",
      "\n",
      "Dataframe shape: (3142, 17)\n",
      "New columns: ['id', 'session', 'electoralTerm', 'firstName', 'lastName', 'politicianId', 'speechContent', 'factionId', 'documentUrl', 'positionShort', 'positionLong', 'date', 'speech_length', 'paragraph_number', 'paragraph_length', 'tokens', 'token_count']\n",
      "\n",
      "Token count statistics:\n",
      "  Min: 2\n",
      "  Max: 4174\n",
      "  Mean: 91.6\n",
      "  Median: 34\n",
      "\n",
      "Sample (first paragraph tokens):\n",
      "  Tokens: shape: (9,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Herr\"\n",
      "\t\"Präsident\"\n",
      "\t\"!\"\n",
      "\t\"Liebe\"\n",
      "\t\"Kolleginnen\"\n",
      "\t\"und\"\n",
      "\t\"Kollegen\"\n",
      "\t\"!\"\n",
      "\t\"Vor\"\n",
      "]\n",
      "  Token count: 9\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all paragraphs using spaCy\n",
    "import spacy\n",
    "import polars as pl\n",
    "\n",
    "# Load German language model\n",
    "try:\n",
    "    nlp = spacy.load('de_core_news_sm')\n",
    "    print(\"spaCy model 'de_core_news_sm' loaded successfully!\")\n",
    "except OSError:\n",
    "    print(\"Model not found. Installing de_core_news_sm...\")\n",
    "    import subprocess\n",
    "    subprocess.run(['python', '-m', 'spacy', 'download', 'de_core_news_sm'])\n",
    "    nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "# Tokenize each paragraph\n",
    "def tokenize_text(text: str) -> list:\n",
    "    \"\"\"Tokenize text using spaCy and return list of tokens.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    return tokens\n",
    "\n",
    "# Add tokens column\n",
    "tokens_list = []\n",
    "for speech_content in df_paragraphs['speechContent']:\n",
    "    tokens = tokenize_text(speech_content)\n",
    "    tokens_list.append(tokens)\n",
    "\n",
    "# Add tokens column to dataframe\n",
    "df_paragraphs = df_paragraphs.with_columns(\n",
    "    pl.Series('tokens', tokens_list)\n",
    ")\n",
    "\n",
    "# Add token count column\n",
    "df_paragraphs = df_paragraphs.with_columns(\n",
    "    pl.col('tokens').list.len().alias('token_count')\n",
    ")\n",
    "\n",
    "print(\"Tokenization complete!\")\n",
    "print(f\"\\nDataframe shape: {df_paragraphs.shape}\")\n",
    "print(f\"New columns: {df_paragraphs.columns}\")\n",
    "\n",
    "print(f\"\\nToken count statistics:\")\n",
    "print(f\"  Min: {df_paragraphs['token_count'].min()}\")\n",
    "print(f\"  Max: {df_paragraphs['token_count'].max()}\")\n",
    "print(f\"  Mean: {df_paragraphs['token_count'].mean():.1f}\")\n",
    "print(f\"  Median: {df_paragraphs['token_count'].median():.0f}\")\n",
    "\n",
    "print(f\"\\nSample (first paragraph tokens):\")\n",
    "print(f\"  Tokens: {df_paragraphs['tokens'][0]}\")\n",
    "print(f\"  Token count: {df_paragraphs['token_count'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66854068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopword Removal\n",
      "Number of German stopwords: 543\n",
      "Sample stopwords: ['ging', 'seines', 'kurz', 'mich', 'wollten', 'du', 'für', 'zur', 'dasselbe', 'mögen', 'sei', 'dürfen', 'allein', 'worden', 'eines', 'je', 'gute', 'geschweige', 'sondern', 'einiges']\n",
      "\n",
      "Stopword removal complete!\n",
      "\n",
      "Dataframe shape: (3142, 19)\n",
      "New columns: ['id', 'session', 'electoralTerm', 'firstName', 'lastName', 'politicianId', 'speechContent', 'factionId', 'documentUrl', 'positionShort', 'positionLong', 'date', 'speech_length', 'paragraph_number', 'paragraph_length', 'tokens', 'token_count', 'tokens_no_stopwords', 'token_count_no_stopwords']\n",
      "\n",
      "Token count comparison:\n",
      "  Original tokens - Mean: 91.6\n",
      "  After stopword removal - Mean: 50.1\n",
      "  Reduction: 45.3%\n",
      "\n",
      "Sample comparison (first paragraph):\n",
      "  Original tokens (9): shape: (9,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Herr\"\n",
      "\t\"Präsident\"\n",
      "\t\"!\"\n",
      "\t\"Liebe\"\n",
      "\t\"Kolleginnen\"\n",
      "\t\"und\"\n",
      "\t\"Kollegen\"\n",
      "\t\"!\"\n",
      "\t\"Vor\"\n",
      "]\n",
      "  After stopword removal (7): shape: (7,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Herr\"\n",
      "\t\"Präsident\"\n",
      "\t\"!\"\n",
      "\t\"Liebe\"\n",
      "\t\"Kolleginnen\"\n",
      "\t\"Kollegen\"\n",
      "\t\"!\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords from tokens\n",
    "import polars as pl\n",
    "import spacy\n",
    "\n",
    "# Load German model (already loaded from previous cell)\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "# Get German stopwords from spaCy\n",
    "german_stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "print(\"Stopword Removal\")\n",
    "print(f\"Number of German stopwords: {len(german_stopwords)}\")\n",
    "print(f\"Sample stopwords: {list(german_stopwords)[:20]}\")\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(tokens: list) -> list:\n",
    "    \"\"\"Remove stopwords from token list.\"\"\"\n",
    "    return [token for token in tokens if token.lower() not in german_stopwords]\n",
    "\n",
    "# Apply stopword removal to all tokens\n",
    "df_paragraphs = df_paragraphs.with_columns(\n",
    "    pl.col('tokens').map_elements(remove_stopwords, return_dtype=pl.List(pl.Utf8)).alias('tokens_no_stopwords')\n",
    ")\n",
    "\n",
    "# Add count of tokens without stopwords\n",
    "df_paragraphs = df_paragraphs.with_columns(\n",
    "    pl.col('tokens_no_stopwords').list.len().alias('token_count_no_stopwords')\n",
    ")\n",
    "\n",
    "print(\"\\nStopword removal complete!\")\n",
    "print(f\"\\nDataframe shape: {df_paragraphs.shape}\")\n",
    "print(f\"New columns: {df_paragraphs.columns}\")\n",
    "\n",
    "print(f\"\\nToken count comparison:\")\n",
    "print(f\"  Original tokens - Mean: {df_paragraphs['token_count'].mean():.1f}\")\n",
    "print(f\"  After stopword removal - Mean: {df_paragraphs['token_count_no_stopwords'].mean():.1f}\")\n",
    "print(f\"  Reduction: {(1 - df_paragraphs['token_count_no_stopwords'].mean() / df_paragraphs['token_count'].mean()) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nSample comparison (first paragraph):\")\n",
    "print(f\"  Original tokens ({df_paragraphs['token_count'][0]}): {df_paragraphs['tokens'][0]}\")\n",
    "print(f\"  After stopword removal ({df_paragraphs['token_count_no_stopwords'][0]}): {df_paragraphs['tokens_no_stopwords'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3f0b8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation and Number Removal\n",
      "Punctuation characters: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "\n",
      "Punctuation and number removal complete!\n",
      "\n",
      "Dataframe shape: (3142, 21)\n",
      "Columns: ['id', 'session', 'electoralTerm', 'firstName', 'lastName', 'politicianId', 'speechContent', 'factionId', 'documentUrl', 'positionShort', 'positionLong', 'date', 'speech_length', 'paragraph_number', 'paragraph_length', 'tokens', 'token_count', 'tokens_no_stopwords', 'token_count_no_stopwords', 'tokens_clean', 'token_count_clean']\n",
      "\n",
      "Token count progression:\n",
      "  Original tokens - Mean: 91.6\n",
      "  After stopword removal - Mean: 50.1\n",
      "  After punct/number removal - Mean: 30.9\n",
      "  Total reduction: 66.3%\n",
      "\n",
      "Sample comparison (first paragraph):\n",
      "  Original tokens (9): shape: (9,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Herr\"\n",
      "\t\"Präsident\"\n",
      "\t\"!\"\n",
      "\t\"Liebe\"\n",
      "\t\"Kolleginnen\"\n",
      "\t\"und\"\n",
      "\t\"Kollegen\"\n",
      "\t\"!\"\n",
      "\t\"Vor\"\n",
      "]\n",
      "  After stopwords (7): shape: (7,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Herr\"\n",
      "\t\"Präsident\"\n",
      "\t\"!\"\n",
      "\t\"Liebe\"\n",
      "\t\"Kolleginnen\"\n",
      "\t\"Kollegen\"\n",
      "\t\"!\"\n",
      "]\n",
      "  After punct/numbers (5): shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Herr\"\n",
      "\t\"Präsident\"\n",
      "\t\"Liebe\"\n",
      "\t\"Kolleginnen\"\n",
      "\t\"Kollegen\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation and numbers from tokens\n",
    "import polars as pl\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Get German stopwords from spaCy (already loaded)\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "print(\"Punctuation and Number Removal\")\n",
    "print(f\"Punctuation characters: {string.punctuation}\")\n",
    "\n",
    "# Function to remove punctuation and numbers\n",
    "def remove_punct_and_numbers(tokens: list) -> list:\n",
    "    \"\"\"Remove punctuation and numbers from token list.\"\"\"\n",
    "    cleaned = []\n",
    "    for token in tokens:\n",
    "        # Check if token contains only punctuation and/or numbers\n",
    "        if all(c in string.punctuation + string.digits for c in token):\n",
    "            # Skip tokens that are purely punctuation or numbers\n",
    "            continue\n",
    "        else:\n",
    "            # Remove punctuation and numbers from within tokens, keep only letters\n",
    "            cleaned_token = re.sub(r'[\\d\\W]', '', token)\n",
    "            if cleaned_token:  # Only add non-empty tokens\n",
    "                cleaned.append(cleaned_token)\n",
    "    return cleaned\n",
    "\n",
    "# Apply punctuation/number removal to tokens without stopwords\n",
    "df_paragraphs = df_paragraphs.with_columns(\n",
    "    pl.col('tokens_no_stopwords').map_elements(remove_punct_and_numbers, return_dtype=pl.List(pl.Utf8)).alias('tokens_clean')\n",
    ")\n",
    "\n",
    "# Add count of clean tokens\n",
    "df_paragraphs = df_paragraphs.with_columns(\n",
    "    pl.col('tokens_clean').list.len().alias('token_count_clean')\n",
    ")\n",
    "\n",
    "print(\"\\nPunctuation and number removal complete!\")\n",
    "print(f\"\\nDataframe shape: {df_paragraphs.shape}\")\n",
    "print(f\"Columns: {df_paragraphs.columns}\")\n",
    "\n",
    "print(f\"\\nToken count progression:\")\n",
    "print(f\"  Original tokens - Mean: {df_paragraphs['token_count'].mean():.1f}\")\n",
    "print(f\"  After stopword removal - Mean: {df_paragraphs['token_count_no_stopwords'].mean():.1f}\")\n",
    "print(f\"  After punct/number removal - Mean: {df_paragraphs['token_count_clean'].mean():.1f}\")\n",
    "print(f\"  Total reduction: {(1 - df_paragraphs['token_count_clean'].mean() / df_paragraphs['token_count'].mean()) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nSample comparison (first paragraph):\")\n",
    "print(f\"  Original tokens ({df_paragraphs['token_count'][0]}): {df_paragraphs['tokens'][0]}\")\n",
    "print(f\"  After stopwords ({df_paragraphs['token_count_no_stopwords'][0]}): {df_paragraphs['tokens_no_stopwords'][0]}\")\n",
    "print(f\"  After punct/numbers ({df_paragraphs['token_count_clean'][0]}): {df_paragraphs['tokens_clean'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d93ac0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization\n",
      "Converting tokens to their base/lemma form using spaCy German model...\n",
      "\n",
      "Lemmatization complete!\n",
      "\n",
      "Dataframe shape: (3142, 23)\n",
      "Columns: ['id', 'session', 'electoralTerm', 'firstName', 'lastName', 'politicianId', 'speechContent', 'factionId', 'documentUrl', 'positionShort', 'positionLong', 'date', 'speech_length', 'paragraph_number', 'paragraph_length', 'tokens', 'token_count', 'tokens_no_stopwords', 'token_count_no_stopwords', 'tokens_clean', 'token_count_clean', 'tokens_lemma', 'token_count_lemma']\n",
      "\n",
      "Token count at each stage:\n",
      "  Original tokens - Mean: 91.6\n",
      "  After stopword removal - Mean: 50.1\n",
      "  After punct/number removal - Mean: 30.9\n",
      "  After lemmatization - Mean: 30.9\n",
      "\n",
      "Sample comparison (first paragraph):\n",
      "  Clean tokens (5): shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Herr\"\n",
      "\t\"Präsident\"\n",
      "\t\"Liebe\"\n",
      "\t\"Kolleginnen\"\n",
      "\t\"Kollegen\"\n",
      "]\n",
      "  Lemmatized (5): shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Herr\"\n",
      "\t\"Präsident\"\n",
      "\t\"Liebe\"\n",
      "\t\"Kollegin\"\n",
      "\t\"Kollege\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "import polars as pl\n",
    "import spacy\n",
    "\n",
    "# Load German model (already loaded from previous cells)\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "print(\"Lemmatization\")\n",
    "print(\"Converting tokens to their base/lemma form using spaCy German model...\")\n",
    "\n",
    "# Function to lemmatize a list of tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Lemmatize a list of tokens by looking up each token individually.\"\"\"\n",
    "    if tokens is None or len(tokens) == 0:\n",
    "        return []\n",
    "    \n",
    "    lemmas = []\n",
    "    for token_text in tokens:\n",
    "        # Process each token through spaCy to get its lemma\n",
    "        doc = nlp(token_text)\n",
    "        if doc and len(doc) > 0:\n",
    "            lemmas.append(doc[0].lemma_)\n",
    "        else:\n",
    "            # If no lemma found, keep the original token\n",
    "            lemmas.append(token_text)\n",
    "    \n",
    "    return lemmas\n",
    "\n",
    "# Apply lemmatization using map_batches with explicit column operation\n",
    "def apply_lemmatization(df):\n",
    "    \"\"\"Apply lemmatization to tokens_clean column.\"\"\"\n",
    "    tokens_lemma_list = []\n",
    "    for tokens in df['tokens_clean']:\n",
    "        lemmas = lemmatize_tokens(tokens)\n",
    "        tokens_lemma_list.append(lemmas)\n",
    "    return tokens_lemma_list\n",
    "\n",
    "# Apply to dataframe\n",
    "lemma_results = apply_lemmatization(df_paragraphs)\n",
    "df_paragraphs = df_paragraphs.with_columns(\n",
    "    pl.Series('tokens_lemma', lemma_results)\n",
    ")\n",
    "\n",
    "# Add count of lemmatized tokens\n",
    "df_paragraphs = df_paragraphs.with_columns(\n",
    "    pl.col('tokens_lemma').list.len().alias('token_count_lemma')\n",
    ")\n",
    "\n",
    "print(\"\\nLemmatization complete!\")\n",
    "print(f\"\\nDataframe shape: {df_paragraphs.shape}\")\n",
    "print(f\"Columns: {df_paragraphs.columns}\")\n",
    "\n",
    "print(f\"\\nToken count at each stage:\")\n",
    "print(f\"  Original tokens - Mean: {df_paragraphs['token_count'].mean():.1f}\")\n",
    "print(f\"  After stopword removal - Mean: {df_paragraphs['token_count_no_stopwords'].mean():.1f}\")\n",
    "print(f\"  After punct/number removal - Mean: {df_paragraphs['token_count_clean'].mean():.1f}\")\n",
    "print(f\"  After lemmatization - Mean: {df_paragraphs['token_count_lemma'].mean():.1f}\")\n",
    "\n",
    "print(f\"\\nSample comparison (first paragraph):\")\n",
    "print(f\"  Clean tokens ({df_paragraphs['token_count_clean'][0]}): {df_paragraphs['tokens_clean'][0]}\")\n",
    "print(f\"  Lemmatized ({df_paragraphs['token_count_lemma'][0]}): {df_paragraphs['tokens_lemma'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9be6e99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved!\n",
      "Output file: ..\\data\\processed\\df_sample_split_preprocessed.parquet\n",
      "File size: 2547.12 KB\n",
      "Total rows (paragraphs): 3142\n",
      "Total columns: 23\n",
      "\n",
      "Dataframe columns:\n",
      "  - id\n",
      "  - session\n",
      "  - electoralTerm\n",
      "  - firstName\n",
      "  - lastName\n",
      "  - politicianId\n",
      "  - speechContent\n",
      "  - factionId\n",
      "  - documentUrl\n",
      "  - positionShort\n",
      "  - positionLong\n",
      "  - date\n",
      "  - speech_length\n",
      "  - paragraph_number\n",
      "  - paragraph_length\n",
      "  - tokens\n",
      "  - token_count\n",
      "  - tokens_no_stopwords\n",
      "  - token_count_no_stopwords\n",
      "  - tokens_clean\n",
      "  - token_count_clean\n",
      "  - tokens_lemma\n",
      "  - token_count_lemma\n"
     ]
    }
   ],
   "source": [
    "# Save preprocessed data to processed folder\n",
    "from pathlib import Path\n",
    "\n",
    "# Define output path\n",
    "processed_dir = Path('../data/processed')\n",
    "processed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save as Parquet (supports nested data like lists)\n",
    "output_file = processed_dir / 'df_sample_split_preprocessed.parquet'\n",
    "df_paragraphs.write_parquet(output_file)\n",
    "\n",
    "print(f\"Preprocessed data saved!\")\n",
    "print(f\"Output file: {output_file}\")\n",
    "print(f\"File size: {output_file.stat().st_size / 1024:.2f} KB\")\n",
    "print(f\"Total rows (paragraphs): {df_paragraphs.shape[0]}\")\n",
    "print(f\"Total columns: {df_paragraphs.shape[1]}\")\n",
    "print(f\"\\nDataframe columns:\")\n",
    "for col in df_paragraphs.columns:\n",
    "    print(f\"  - {col}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parl_speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
